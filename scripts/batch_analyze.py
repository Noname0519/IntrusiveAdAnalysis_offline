import os
import json
import shutil
import csv
import pandas as pd
from datetime import datetime
import time

class dynamic_graph():
    def __init__(self, js_path=None, json_path=None):

        self.ad_screenshots_dir = "F:\\test\\ad_screenshots"
        self.ad_ui_files_dir = "F:\\test\\ad_ui_files"

        if json_path is None:
            self.json_path = js_path.replace("js", "json")
        else:
            self.json_path = json_path
        if js_path is not None:
            ret = self.change_js_to_json(js_path, self.json_path)
            
        self.state = {}
        self.state_edge = []
        self.state_edge_json = []
        self.activity = {}
        self.edge = []
        try:
            with open(self.json_path, encoding='utf-8') as f:
                j = json.load(f)
        except Exception as e:
            print(f"åŠ è½½UTGå¤±è´¥: {e}")
            return

        self.raw_utg = j  # ä¿å­˜åŸå§‹utgï¼Œæ–¹ä¾¿åç»­å¢å¼º

        for js_node in j.setdefault('nodes', []):
            id = js_node['id']
            act = js_node['activity']
            self.state[id] = js_node
            self.activity.setdefault(act, {}).setdefault('state', []).append(id)

        for js_transition in j.setdefault('edges', []):
            src = js_transition['from']
            dst = js_transition['to']
            trigger = js_transition['events']

            src_ad_related = self.state[src].get("is_ad_related", False)
            src_ad_feature = self.state[src].get("ad_feature", {})
            src_is_external = self.state[src].get("is_external_site", False)

            dst_ad_related = self.state[dst].get("is_ad_related", False)
            dst_ad_feature = self.state[dst].get("ad_feature", {})
            dst_is_external = self.state[dst].get("is_external_site", False)
 
            # self.state[src].setdefault('src', {})[dst] = {'is_ad_related': is_ad_related, 'events': trigger}
            # self.state[dst].setdefault('dst', {})[src] = {'is_ad_related': is_ad_related, 'events': trigger}

            # åœ¨ src èŠ‚ç‚¹é‡Œè®°å½• dst é‚»å±…ï¼ŒåŒæ—¶å†™å…¥å¹¿å‘Šæ ‡è®°
            self.state[src].setdefault('dst', {})[dst] = {
                'is_ad_related': dst_ad_related, 
                'is_external_site': dst_is_external,
                'events': trigger,
                'ad_feature': dst_ad_feature
            }

            # åœ¨ dst èŠ‚ç‚¹é‡Œè®°å½• src é‚»å±…ï¼ŒåŒæ—¶å†™å…¥å¹¿å‘Šæ ‡è®°
            self.state[dst].setdefault('src', {})[src] = {
                'is_ad_related': src_ad_related, 
                'is_external_site': src_is_external,
                'events': trigger,
                'ad_feature': src_ad_feature
            }

            self.state_edge.append([src, dst])
            self.state_edge_json.append(js_transition)
            src_act = self.state[src]['activity']
            dst_act = self.state[dst]['activity']
            if src_act != dst_act:
                edge = [src_act, dst_act]
                if edge not in self.edge:
                    self.edge.append(edge)
                    self.activity[src_act].setdefault('src', {})[dst_act] = {'trigger': trigger, 'weight': 1}
                    self.activity[dst_act].setdefault('dst', {})[src_act] = {'trigger': trigger, 'weight': 1}
                else:
                    self.activity[src_act]['src'][dst_act]['weight'] += 1
                    self.activity[dst_act]['dst'][src_act]['weight'] += 1
                    self.activity[src_act]['src'][dst_act]['trigger'] += trigger
                    self.activity[dst_act]['dst'][src_act]['trigger'] += trigger

    def change_js_to_json(self, js_path, json_path, encoding="utf-8"):
        
        with open(js_path, encoding=encoding) as f:
            js_content = f.read().strip()
        

        # å»æ‰å‰ç¼€ "var utg ="
        if js_content.startswith("var utg"):
            js_content = js_content[js_content.index("{"):]  

        # å»æ‰ç»“å°¾çš„åˆ†å·
        if js_content.endswith(";"):
            js_content = js_content[:-1]

        # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(json_path), exist_ok=True)

        with open(json_path, "w", encoding=encoding) as f:
            f.write(js_content)
        print(f"[+] è½¬æ¢å®Œæˆ: {json_path}")

    def _load_false_positive_keywords(self, file_path):
        """
        åŠ è½½è¯¯è¯†åˆ«å…³é”®è¯æ–‡ä»¶
        """
        false_positive_keywords = set()
        
        if os.path.exists(file_path):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Š
                        if line and not line.startswith('#'):
                            false_positive_keywords.add(line)
                print(f"[+] å·²åŠ è½½ {len(false_positive_keywords)} ä¸ªè¯¯è¯†åˆ«å…³é”®è¯")
            except Exception as e:
                print(f"[-] åŠ è½½è¯¯è¯†åˆ«å…³é”®è¯æ–‡ä»¶å¤±è´¥: {e}")
                # ä½¿ç”¨é»˜è®¤çš„è¯¯è¯†åˆ«å…³é”®è¯
                false_positive_keywords = set([
                    "loading", "leading", "headline", "header", "footer", "bottom", 
                    "top", "progress", "loader", "indicator", "status", "bar", "banner_generic"
                ])
        else:
            print(f"[-] è¯¯è¯†åˆ«å…³é”®è¯æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            # ä½¿ç”¨é»˜è®¤çš„è¯¯è¯†åˆ«å…³é”®è¯
            false_positive_keywords = set([
                "loading", "leading", "headline", "header", "footer", "bottom", 
                "top", "progress", "loader", "indicator", "status", "bar", "banner_generic"
            ])
        
        return false_positive_keywords

    # def enhance_utg(self, root_dir, keywords=None, save_back=True):
    #     """
    #     éå†UTGèŠ‚ç‚¹ï¼Œæ ¹æ®å¯¹åº”çš„ state.json æ£€æµ‹å¹¿å‘Šç›¸å…³è§†å›¾ï¼Œå¢å¼ºèŠ‚ç‚¹ä¿¡æ¯ï¼Œå¹¶è®°å½•æ—¥å¿—
    #     """
    #     log_file=os.path.join(root_dir, "enhanced_log.txt")

    #     false_positive_file = os.path.join(root_dir, "false_positive_keywords.txt")
    #     false_positive_keywords = self.

    #     if keywords is None:
    #         keywords = ["ad_contain", "ad_view", "advertisement", "å¹¿å‘Š", "ad_icon", "ad_title", "adView"]
        
    #     false_positive_keywords = ["load", "loading", "lead", "leading", "adapter", "adopt", "adapt"]
    #     false_positive_nodes = []

    #     nodes = self.raw_utg.get("nodes", [])
    #     log_entries = []

    #     false_positive_nodes = []
    #     # check false_positive
    #     for node in nodes:
    #         if node.get("is_ad_related", False):
    #             ad_features = node.get("ad_feature", [])
    #             is_false_positive = False

    #             for feature in ad_features:
    #                 if isinstance(feature, dict):
    #                     for value in feature.values():
    #                         if any(fp_kw in str(value).lower() for fp_kw in false_positive_keywords):
    #                             is_false_positive = True
    #                             break
    #                 elif any(fp_kw in str(feature).lower() for fp_kw in false_positive_keywords):
    #                     is_false_positive = True

    #             if is_false_positive:
    #                 false_positive_nodes.append(node["id"])

    #                 node["is_ad_related"] = False
    #                 if "ad_feature" in node:
    #                     node["ad_feature"] = []
    #                 if "ad_format" in node:
    #                     node["ad_format"] = []

    #                 log_line = f"âš ï¸  Node {node['id']} è¢«ä¿®æ­£ï¼šåŸå¹¿å‘Šè¯†åˆ«ä¸ºè¯¯è¯†åˆ«ï¼ˆåŒ…å«è¿‡æ»¤å…³é”®è¯ï¼‰"
    #                 print("[-] " + log_line)
    #                 log_entries.append(log_line)

    #     for node in nodes:
    #         #if not node.get("is_ad_related", False): # æš‚æ—¶æ³¨é‡Šï¼Œæœ‰äº›ad_feature & ad_formatå­˜ä¸ä¸‹æ¥

    #         if node["id"] in false_positive_nodes:
    #             continue    
            
    #         image_path = node.get("image")
            
    #         if not image_path:
    #             continue

    #         # ç”Ÿæˆå¯¹åº”çš„ state.json æ–‡ä»¶è·¯å¾„
    #         # state_json_name = os.path.basename(image_path).replace("screen", "state").replace(".png", ".json")
    #         # state_json_path = os.path.join(root_dir, state_json_name)

    #         image_path = image_path.replace("\\", "/")
    #         state_json_name = os.path.basename(image_path).replace("screen", "state").replace(".png", ".json")
            
    #         state_json_path = os.path.normpath(os.path.join(root_dir,"states", state_json_name))

    #         if not os.path.exists(state_json_path):
    #             continue

    #         try:
    #             with open(state_json_path, "r", encoding="utf-8") as sf:
    #                 state_data = json.load(sf)
    #         except Exception as e:
    #             print(f"è¯»å– {state_json_path} å¤±è´¥: {e}")
    #             continue

    #         views = state_data.get("views", [])
    #         ad_features = []
    #         ad_formats = set()

    #         for view in views:

    #             # skip modified nodes:
    #             has_false_positive = False
    #             # if node["id"] in false_positive_nodes:
    #             #     continue

    #             # if view.get("ad_feature"):
    #             #     ad_features.append(view["ad_feature"])

    #             #     if view.get("ad_format") is not None:
    #             #         node["ad_format"] = view["ad_format"]
    #             #         print(node["ad_format"])
    #             #         continue

    #             for field in ["resource_id", "text", "class"]:
                    
    #                 if field in view and view[field]:
    #                     if field in view and view[field]:
    #                         field_value = str(view[field]).lower()
    #                         if any(fp_kw in field_value for fp_kw in false_positive_keywords):
    #                             has_false_positive = True
    #                             break

    #                     # for kw in keywords:
                            
    #                     #     if kw.lower() in str(view[field]).lower():
    #                     #         #print(str(view[field]))
    #                     #         ad_features.append({field: view[field]})
    #                     #         #print(node["state_str"])

    #             if has_false_positive:
    #                 continue

    #             if view.get("ad_feature"):
    #                 ad_features.append(view["ad_feature"])

    #                 if view.get("ad_format") is not None:
    #                     ad_formats.add(view["ad_format"])
                
    #             # å…³é”®è¯åŒ¹é…ï¼ˆæ’é™¤è¯¯è¯†åˆ«åï¼‰
    #             for field in ["resource_id", "text"]:
    #                 if field in view and view[field]:
    #                     for kw in keywords:
    #                         field_value = str(view[field]).lower()
    #                         kw_lower = kw.lower()
                            
    #                         # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›®æ ‡å…³é”®è¯ä½†ä¸åŒ…å«è¯¯è¯†åˆ«å…³é”®è¯
    #                         if (kw_lower in field_value and 
    #                             not any(fp_kw in field_value for fp_kw in false_positive_keywords)):
    #                             feature_entry = {field: view[field], "matched_keyword": kw}
    #                             if feature_entry not in ad_features:
    #                                 ad_features.append(feature_entry)

    #         # å¤„ç†å¹¿å‘Šæ ¼å¼
    #         if ad_formats:
    #             node["ad_format"] = list(ad_formats)
    #             if len(ad_formats) == 1:
    #                 node["ad_format"] = ad_formats.pop()

    #         if ad_features:
    #             was_ad_related = node.get("is_ad_related", False)
    #             node["is_ad_related"] = True
    #             node["ad_feature"] = ad_features

    #             if was_ad_related:
    #                 # è®°å½•æ—¥å¿—
    #                 log_line = f"Node {node['id']} ({image_path}) è¢«æ ‡è®°ä¸ºå¹¿å‘Šç›¸å…³, ç‰¹å¾: {ad_features}"
    #                 print("[+] " + log_line)
    #                 log_entries.append(log_line)

    #     # æ›´æ–° self.state é‡Œçš„èŠ‚ç‚¹
    #     for node in nodes:
    #         self.state[node["id"]] = node

    #     # ä¿å­˜å¢å¼ºåçš„UTG
    #     if save_back:
    #         enhanced_utg_path = os.path.join(root_dir, "enhanced_utg.json")
    #         with open(enhanced_utg_path, "w", encoding="utf-8") as f:
    #             json.dump(self.raw_utg, f, indent=2, ensure_ascii=False)
    #         print("[+] å¢å¼ºUTGå·²ä¿å­˜åˆ° enhanced_utg.json")

    #     # ä¿å­˜æ—¥å¿—
    #     if log_entries:
    #         with open(log_file, "a", encoding="utf-8") as lf:
    #             lf.write("\n".join(log_entries) + "\n")
    #         print(f"[+] æ—¥å¿—å·²å†™å…¥ {log_file}")

    def enhance_utg(self, root_dir, keywords=None, save_back=True):
        """
        éå†UTGèŠ‚ç‚¹ï¼Œæ ¹æ®å¯¹åº”çš„ state.json æ£€æµ‹å¹¿å‘Šç›¸å…³è§†å›¾ï¼Œå¢å¼ºèŠ‚ç‚¹ä¿¡æ¯ï¼Œå¹¶è®°å½•æ—¥å¿—
        å¢åŠ è¯¯è¯†åˆ«è¿‡æ»¤å’Œç°æœ‰èŠ‚ç‚¹æ£€æŸ¥é€»è¾‘
        """
        log_file = os.path.join(root_dir, "enhanced_log.txt")

        # å…¨å±€å˜é‡å®šä¹‰ï¼ˆå¯ä»¥åœ¨ç±»åˆå§‹åŒ–æ—¶å®šä¹‰ï¼‰
        if not hasattr(self, 'ad_screenshots_dir'):
            self.ad_screenshots_dir = os.path.join(root_dir, "ad_screenshots")
        if not hasattr(self, 'ad_ui_files_dir'):
            self.ad_ui_files_dir = os.path.join(root_dir, "ad_ui_files")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.ad_screenshots_dir, exist_ok=True)
        os.makedirs(self.ad_ui_files_dir, exist_ok=True)

        # if keywords is None:
        #     keywords = ["ad_contain", "ad_view", "advertisement", "å¹¿å‘Š", "ad_icon", "ad_title", "adView"]
        
        # # è¯¯è¯†åˆ«å…³é”®è¯è¿‡æ»¤åˆ—è¡¨
        # false_positive_keywords = ["loading", "leading", "headline", "header", "footer", "bottom", "top", 
        #                         "progress", "loader", "indicator", "status", "bar", "banner_generic"]

        # false_positive_file = os.path.join(root_dir, "false_positive_keywords.txt")
        false_positive_file = "D:\\NKU\\Work\\Work2\\datasets\\androzoo\\false_positive_keywords.txt"
        false_positive_keywords = self._load_false_positive_keywords(false_positive_file)
        if keywords is None:
            keywords = ["ad_contain", "ad_view", "advertisement", "å¹¿å‘Š", "ad_icon", "ad_title", "adView"]
        
        
        nodes = self.raw_utg.get("nodes", [])
        log_entries = []

        # åˆå§‹åŒ–ç»Ÿè®¡è®¡æ•°å™¨
        stats = {
            "total_nodes": len(nodes),
            "original_ad_nodes": 0,  # åŸå§‹å¹¿å‘ŠèŠ‚ç‚¹æ•°
            "false_positive_corrected": 0,  # è¯¯è¯†åˆ«ä¿®æ­£æ•°
            "new_ad_nodes": 0,  # æ–°æ ‡è®°å¹¿å‘ŠèŠ‚ç‚¹æ•°
            "confirmed_ad_nodes": 0,  # ç¡®è®¤å¹¿å‘ŠèŠ‚ç‚¹æ•°
            "cleared_ad_nodes": 0,  # æ¸…é™¤å¹¿å‘Šæ ‡è®°èŠ‚ç‚¹æ•°
            "ad_nodes_with_screenshots": 0,  # æˆåŠŸå¤åˆ¶æˆªå›¾çš„å¹¿å‘ŠèŠ‚ç‚¹æ•°
            "ad_nodes_with_ui_files": 0,  # æˆåŠŸå¤åˆ¶UIæ–‡ä»¶çš„å¹¿å‘ŠèŠ‚ç‚¹æ•°
            "ad_features_found": 0,  # å‘ç°çš„å¹¿å‘Šç‰¹å¾æ€»æ•°
        }

        # é¦–å…ˆç»Ÿè®¡åŸå§‹å¹¿å‘ŠèŠ‚ç‚¹æ•°
        stats["original_ad_nodes"] = sum(1 for node in nodes if node.get("is_ad_related", False))
        
        # é¦–å…ˆæ£€æŸ¥ç°æœ‰è¢«è¯†åˆ«ä¸ºad_relatedçš„èŠ‚ç‚¹æ˜¯å¦å­˜åœ¨è¯¯è¯†åˆ«
        false_positive_nodes = []
        for node in nodes:
            if node.get("is_ad_related", False):
                ad_features = node.get("ad_feature", [])
                is_false_positive = False
                
                # æ£€æŸ¥ad_featureä¸­æ˜¯å¦åŒ…å«è¯¯è¯†åˆ«å…³é”®è¯
                for feature in ad_features:
                    feature_str = str(feature).lower()
                    if any(fp_kw.lower() in feature_str for fp_kw in false_positive_keywords):
                        is_false_positive = True
                        break
                
                if is_false_positive:
                    false_positive_nodes.append(node["id"])
                    # ä¿®æ­£è¯¯è¯†åˆ«
                    node["is_ad_related"] = False
                    if "ad_feature" in node:
                        del node["ad_feature"]
                    if "ad_format" in node:
                        del node["ad_format"]
                    
                    log_line = f"âš ï¸  Node {node['id']} è¢«ä¿®æ­£ï¼šåŸå¹¿å‘Šè¯†åˆ«ä¸ºè¯¯è¯†åˆ«ï¼ˆåŒ…å«è¿‡æ»¤å…³é”®è¯ï¼‰"
                    print("[-] " + log_line)
                    log_entries.append(log_line)
                    stats["false_positive_corrected"] += 1

        # å¤„ç†æ¯ä¸ªèŠ‚ç‚¹
        for node in nodes:
            # è·³è¿‡å·²ç»è¢«ä¿®æ­£çš„è¯¯è¯†åˆ«èŠ‚ç‚¹
            if node["id"] in false_positive_nodes:
                continue
                
            image_path = node.get("image") # states\screen_2025-10-15_110502.png
            
            if not image_path:
                continue

            image_path = image_path.replace("\\", "/")
            state_image_name = os.path.basename(image_path)
            state_image_path = os.path.normpath(os.path.join(root_dir, "states", state_image_name))
            state_json_name = os.path.basename(image_path).replace("screen", "state").replace(".png", ".json")
            state_json_path = os.path.normpath(os.path.join(root_dir, "states", state_json_name))

            if not os.path.exists(state_json_path):
                continue

            try:
                with open(state_json_path, "r", encoding="utf-8") as sf:
                    state_data = json.load(sf)
            except Exception as e:
                print(f"è¯»å– {state_json_path} å¤±è´¥: {e}")
                continue

            views = state_data.get("views", [])
            ad_features = []
            ad_formats = set()

            for view in views:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«è¯¯è¯†åˆ«å…³é”®è¯
                has_false_positive = False
                for field in ["resource_id", "text", "class", "content_description"]:
                    if field in view and view[field]:
                        field_value = str(view[field]).lower()
                        if any(fp_kw in field_value for fp_kw in false_positive_keywords):
                            has_false_positive = True
                            break
                
                # å¦‚æœåŒ…å«è¯¯è¯†åˆ«å…³é”®è¯ï¼Œè·³è¿‡è¯¥view
                if has_false_positive:
                    continue
                    
                # if view.get("ad_feature"):
                #     ad_features.append(view["ad_feature"])

                #     if view.get("ad_format") is not None:
                #         ad_formats.add(view["ad_format"])
                if view.get("ad_feature"):
                    # å†æ¬¡æ£€æŸ¥å¹¿å‘Šç‰¹å¾æœ¬èº«æ˜¯å¦åŒ…å«è¯¯è¯†åˆ«è¯
                    ad_feature_str = str(view["ad_feature"]).lower()
                    if not any(fp_kw.lower() in ad_feature_str for fp_kw in false_positive_keywords):
                        ad_features.append(view["ad_feature"])
                        stats["ad_features_found"] += 1

                    if view.get("ad_format") is not None:
                        ad_formats.add(view["ad_format"])

                # å…³é”®è¯åŒ¹é…ï¼ˆæ’é™¤è¯¯è¯†åˆ«åï¼‰
                for field in ["resource_id", "text"]:
                    if field in view and view[field]:

                        field_value = str(view[field]).lower()
                        has_fp_in_field = any(fp_kw.lower() in field_value for fp_kw in false_positive_keywords)
                        if has_fp_in_field:
                            continue

                        for kw in keywords:
                            # field_value = str(view[field]).lower()
                            kw_lower = kw.lower()
                            if kw_lower in field_value:
                                feature_entry = {
                                    field: view[field],
                                    "matched_keyword": kw,
                                    "view_class": view.get("class", "Unknown")
                                }
                            
                                # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›®æ ‡å…³é”®è¯ä½†ä¸åŒ…å«è¯¯è¯†åˆ«å…³é”®è¯
                                # if (kw_lower in field_value and 
                                #     not any(fp_kw in field_value for fp_kw in false_positive_keywords)):
                                #     feature_entry = {field: view[field], "matched_keyword": kw}
                                #     if feature_entry not in ad_features:
                                #         ad_features.append(feature_entry)
                                if not any(fe.get(field) == view[field] and fe.get("matched_keyword") == kw 
                                        for fe in ad_features):
                                    ad_features.append(feature_entry)
                                    stats["ad_features_found"] += 1

            # å¤„ç†å¹¿å‘Šæ ¼å¼
            if ad_formats:
                node["ad_format"] = list(ad_formats)
                if len(ad_formats) == 1:
                    node["ad_format"] = ad_formats.pop()

            # åªæœ‰åœ¨æœ‰æœ‰æ•ˆå¹¿å‘Šç‰¹å¾æ—¶æ‰æ ‡è®°ä¸ºå¹¿å‘Šç›¸å…³
            if ad_features:
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°å‘ç°çš„å¹¿å‘ŠèŠ‚ç‚¹
                was_ad_related = node.get("is_ad_related", False)
                node["is_ad_related"] = True
                node["ad_feature"] = ad_features

                if was_ad_related:
                    log_line = f"âœ… Node {node['id']} ({state_image_name}) ç¡®è®¤å¹¿å‘Šç›¸å…³, ç‰¹å¾: {len(ad_features)}ä¸ª"
                    stats["confirmed_ad_nodes"] += 1
                else:
                    log_line = f"ğŸ¯ Node {node['id']} ({state_image_name}) æ–°æ ‡è®°ä¸ºå¹¿å‘Šç›¸å…³, ç‰¹å¾: {len(ad_features)}ä¸ª"
                    stats["new_ad_nodes"] += 1

                print("[+] " + log_line)
                log_entries.append(log_line)
                
                # è¯¦ç»†ç‰¹å¾æ—¥å¿—
                for i, feature in enumerate(ad_features, 1):
                    detail_log = f"   ç‰¹å¾{i}: {feature}"
                    log_entries.append(detail_log)

                # å¤åˆ¶æˆªå›¾å’ŒUIæ–‡ä»¶
                try:
                    # å¤åˆ¶æˆªå›¾
                    if os.path.exists(state_image_path):
                        screenshot_dest = os.path.join(self.ad_screenshots_dir, state_image_name)
                        shutil.copy2(state_image_path, screenshot_dest)
                        stats["ad_nodes_with_screenshots"] += 1
                    
                    # å¤åˆ¶UIæ–‡ä»¶ï¼ˆstate.jsonï¼‰
                    if os.path.exists(state_json_path):
                        # ui_file_dest = os.path.join(self.ad_ui_files_dir, f"node_{node['id']}.json")
                        ui_file_dest = os.path.join(self.ad_ui_files_dir, state_json_name)
                        shutil.copy2(state_json_path, ui_file_dest)
                        stats["ad_nodes_with_ui_files"] += 1
                        
                except Exception as e:
                    print(f"[!] å¤åˆ¶å¹¿å‘ŠèŠ‚ç‚¹æ–‡ä»¶å¤±è´¥: {e}")

            else:
                # å¦‚æœæ²¡æœ‰å¹¿å‘Šç‰¹å¾ä½†ä¹‹å‰è¢«æ ‡è®°ä¸ºå¹¿å‘Šç›¸å…³ï¼Œè¿›è¡Œæ¸…ç†
                if node.get("is_ad_related", False):
                    node["is_ad_related"] = False
                    if "ad_feature" in node:
                        del node["ad_feature"]
                    if "ad_format" in node:
                        del node["ad_format"]
                    
                    log_line = f"ğŸ§¹ Node {node['id']} å¹¿å‘Šæ ‡è®°è¢«æ¸…é™¤ï¼šæœªå‘ç°æœ‰æ•ˆå¹¿å‘Šç‰¹å¾"
                    print("[-] " + log_line)
                    log_entries.append(log_line)
                    stats["cleared_ad_nodes"] += 1

        # æ›´æ–° self.state é‡Œçš„èŠ‚ç‚¹
        for node in nodes:
            self.state[node["id"]] = node

        # ä¿å­˜å¢å¼ºåçš„UTG
        if save_back:
            enhanced_utg_path = os.path.join(root_dir, "enhanced_utg.json")
            with open(enhanced_utg_path, "w", encoding="utf-8") as f:
                json.dump(self.raw_utg, f, indent=2, ensure_ascii=False)
            print("[+] å¢å¼ºUTGå·²ä¿å­˜åˆ° enhanced_utg.json")

        # ä¿å­˜æ—¥å¿—
        if log_entries:
            with open(log_file, "a", encoding="utf-8") as lf:
                lf.write("\n" + "="*50 + "\n")
                lf.write(f"å¢å¼ºæ—¥å¿— - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                lf.write("="*50 + "\n")
                lf.write("\n".join(log_entries) + "\n")
            print(f"[+] æ—¥å¿—å·²å†™å…¥ {log_file}")
        
        # ä¿å­˜å¹¿å‘ŠèŠ‚ç‚¹ä¿¡æ¯åˆ°å•ç‹¬æ–‡ä»¶
        self._save_ad_nodes_info(root_dir, nodes)
            
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        final_ad_nodes = sum(1 for node in nodes if node.get("is_ad_related", False))
        ad_edges_count = sum(1 for edge in self.state_edge_json if edge.get("is_ad_related", False))

        print(f"\nğŸ“Š å¢å¼ºUTGç»Ÿè®¡ä¿¡æ¯:")
        print(f"   æ€»èŠ‚ç‚¹æ•°: {stats['total_nodes']}")
        print(f"   åŸå§‹å¹¿å‘ŠèŠ‚ç‚¹: {stats['original_ad_nodes']}")
        print(f"   è¯¯è¯†åˆ«ä¿®æ­£: {stats['false_positive_corrected']}")
        print(f"   æ–°æ ‡è®°å¹¿å‘ŠèŠ‚ç‚¹: {stats['new_ad_nodes']}")
        print(f"   ç¡®è®¤å¹¿å‘ŠèŠ‚ç‚¹: {stats['confirmed_ad_nodes']}")
        print(f"   æ¸…é™¤å¹¿å‘Šæ ‡è®°: {stats['cleared_ad_nodes']}")
        print(f"   æœ€ç»ˆå¹¿å‘ŠèŠ‚ç‚¹: {final_ad_nodes}")
        print(f"   å¹¿å‘Šç‰¹å¾å‘ç°: {stats['ad_features_found']}")
        print(f"   å¤åˆ¶æˆªå›¾æˆåŠŸ: {stats['ad_nodes_with_screenshots']}")
        print(f"   å¤åˆ¶UIæ–‡ä»¶æˆåŠŸ: {stats['ad_nodes_with_ui_files']}")
        print(f"   å¹¿å‘Šç›¸å…³è¾¹: {ad_edges_count}")

    def _save_ad_nodes_info(self, root_dir, nodes):
        """
        ä¿å­˜å¹¿å‘ŠèŠ‚ç‚¹ä¿¡æ¯åˆ°å•ç‹¬æ–‡ä»¶
        """
        ad_nodes_info = []
        
        for node in nodes:
            if node.get("is_ad_related", False):
                node_info = {
                    "node_id": node["id"],
                    "image_path": node.get("image", ""),
                    "package_name": node.get("package_name", ""),
                    "is_ad_related": True,
                    "ad_features": node.get("ad_feature", []),
                    "ad_format": node.get("ad_format", ""),
                    "screenshot_path": os.path.join(self.ad_screenshots_dir, f"node_{node['id']}.png"),
                    "ui_file_path": os.path.join(self.ad_ui_files_dir, f"node_{node['id']}.json"),
                    "timestamp": datetime.now().isoformat()
                }
                ad_nodes_info.append(node_info)
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        ad_info_file = os.path.join(root_dir, "ad_nodes_info.json")
        with open(ad_info_file, "w", encoding="utf-8") as f:
            json.dump(ad_nodes_info, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"[+] å¹¿å‘ŠèŠ‚ç‚¹ä¿¡æ¯å·²ä¿å­˜åˆ°: {ad_info_file}")

    def _rebuild_edges(self):
        """
        é‡æ–°æ„å»ºè¾¹ä¿¡æ¯ï¼ŒåŸºäºå¢å¼ºåçš„èŠ‚ç‚¹å±æ€§æ›´æ–°è¾¹çš„å¹¿å‘Šç›¸å…³æ ‡è®°
        """
        # æ¸…ç©ºç°æœ‰çš„è¾¹ä¿¡æ¯
        self.state_edge = []
        self.state_edge_json = []
        
        # æ¸…ç©ºèŠ‚ç‚¹ä¸­çš„é‚»å±…ä¿¡æ¯
        for node_id in self.state:
            if 'src' in self.state[node_id]:
                self.state[node_id]['src'] = {}
            if 'dst' in self.state[node_id]:
                self.state[node_id]['dst'] = {}
        
        # é‡æ–°æ„å»ºè¾¹
        for js_transition in self.raw_utg.setdefault('edges', []):
            src = js_transition['from']
            dst = js_transition['to']
            trigger = js_transition['events']

            # ä½¿ç”¨å¢å¼ºåçš„èŠ‚ç‚¹å±æ€§
            src_ad_related = self.state[src].get("is_ad_related", False)
            src_ad_feature = self.state[src].get("ad_feature", {})
            src_is_external = self.state[src].get("is_external_site", False)

            dst_ad_related = self.state[dst].get("is_ad_related", False)
            dst_ad_feature = self.state[dst].get("ad_feature", {})
            dst_is_external = self.state[dst].get("is_external_site", False)

            # æ›´æ–°è¾¹çš„å¹¿å‘Šç›¸å…³æ ‡è®°
            js_transition['src_ad_related'] = src_ad_related
            js_transition['dst_ad_related'] = dst_ad_related
            js_transition['is_ad_related'] = src_ad_related or dst_ad_related

            # åœ¨ src èŠ‚ç‚¹é‡Œè®°å½• dst é‚»å±…ï¼Œä½¿ç”¨æ›´æ–°åçš„å¹¿å‘Šå±æ€§
            self.state[src].setdefault('dst', {})[dst] = {
                'is_ad_related': dst_ad_related, 
                'is_external_site': dst_is_external,
                'events': trigger,
                'ad_feature': dst_ad_feature
            }

            # åœ¨ dst èŠ‚ç‚¹é‡Œè®°å½• src é‚»å±…ï¼Œä½¿ç”¨æ›´æ–°åçš„å¹¿å‘Šå±æ€§
            self.state[dst].setdefault('src', {})[src] = {
                'is_ad_related': src_ad_related, 
                'is_external_site': src_is_external,
                'events': trigger,
                'ad_feature': src_ad_feature
            }

            self.state_edge.append([src, dst])
            self.state_edge_json.append(js_transition)
            
            # æ›´æ–°æ´»åŠ¨çº§åˆ«çš„è¾¹
            src_act = self.state[src]['activity']
            dst_act = self.state[dst]['activity']
            if src_act != dst_act:
                edge = [src_act, dst_act]
                if edge not in self.edge:
                    self.edge.append(edge)
                    self.activity[src_act].setdefault('src', {})[dst_act] = {
                        'trigger': trigger, 
                        'weight': 1,
                        'is_ad_related': src_ad_related or dst_ad_related
                    }
                    self.activity[dst_act].setdefault('dst', {})[src_act] = {
                        'trigger': trigger, 
                        'weight': 1,
                        'is_ad_related': src_ad_related or dst_ad_related
                    }
                else:
                    self.activity[src_act]['src'][dst_act]['weight'] += 1
                    self.activity[dst_act]['dst'][src_act]['weight'] += 1
                    self.activity[src_act]['src'][dst_act]['trigger'] += trigger
                    self.activity[dst_act]['dst'][src_act]['trigger'] += trigger


        # def enhance_utg(self, root_dir, keywords=None, save_back=True):
        #     """
        #     éå†UTGèŠ‚ç‚¹ï¼Œæ ¹æ®å¯¹åº”çš„ state.json æ£€æµ‹å¹¿å‘Šç›¸å…³è§†å›¾ï¼Œå¢å¼ºèŠ‚ç‚¹ä¿¡æ¯
        #     """
        #     if keywords is None:
        #         keywords = ["ad_contain", "ad_view", "ad_banner", "advertisement"]

        #     nodes = self.raw_utg.get("nodes", [])
        #     for node in nodes:
        #         if not node.get("is_ad_related", False):
        #             image_path = node.get("image")
        #             if not image_path:
        #                 continue

        #             # ç”Ÿæˆå¯¹åº”çš„ state.json æ–‡ä»¶è·¯å¾„
        #             state_json_name = os.path.basename(image_path).replace("screen", "state").replace(".png", ".json")
        #             state_json_path = os.path.join(root_dir, state_json_name)

        #             if not os.path.exists(state_json_path):
        #                 continue

        #             try:
        #                 with open(state_json_path, "r", encoding="utf-8") as sf:
        #                     state_data = json.load(sf)
        #             except Exception as e:
        #                 print(f"è¯»å– {state_json_path} å¤±è´¥: {e}")
        #                 continue

        #             views = state_data.get("views", [])
        #             ad_features = []

        #             for view in views:
        #                 for field in ["resource_id", "text"]:
        #                     if field in view and view[field]:
        #                         for kw in keywords:
        #                             if kw.lower() in str(view[field]).lower():
        #                                 ad_features.append({field: view[field]})

        #             if ad_features:
        #                 node["is_ad_related"] = True
        #                 node["ad_feature"] = ad_features

        #     # æ›´æ–° self.state é‡Œçš„èŠ‚ç‚¹
        #     for node in nodes:
        #         self.state[node["id"]] = node

        #     if save_back:
        #         with open("enhanced_utg.json", "w", encoding="utf-8") as f:
        #             json.dump(self.raw_utg, f, indent=2, ensure_ascii=False)
        #         print("[+] å¢å¼ºUTGå·²ä¿å­˜åˆ° enhanced_utg.json")


"""
def analyze(path):
    print("[+] test ... " + path)
    utg_fail_count = 0
    ad_count = 0
    summary = 0
    type2 = {}
    type3 = {}
    type4 = {}
    type5 = {}
    type6 = {}

    type2_list = []
    type3_list = []
    type4_list = []
    type5_list = []
    type6_list = []

    results = {
        'total_apk': 0,
        'apks_with_ad_states': 0,
        'apks_with_frida_logs': 0,
        'ad_state_counts': 0,

    }

    result = {
        "package_name": None,
        "sha": None,
        "has_ad": False
    }

    folders = os.listdir(path)
    for app_dir in folders:
        
        apk_path = os.path.join(path, app_dir)
        if not os.path.isdir(apk_path):
            continue
        
        utg_path = os.path.join(apk_path, "utg.js")
        print("utg_path: " + utg_path)
        if not os.path.exists(utg_path):
            print(f"[!] utg.js ä¸å­˜åœ¨: {utg_path}")
            utg_fail_count = utg_fail_count + 1
            continue
        
        # utg_json_path = os.path.join(apk_path, "utg.json")
        # if not os.path.exists(utg_json_path):
        #     print(f"[!] utg.json ä¸å­˜åœ¨: {utg_json_path}")
        #     continue

        utg = dynamic_graph(js_path=utg_path)
        utg.enhance_utg(apk_path)
        enhance_utg_path = os.path.join(apk_path, "enhanced_utg.json")
        enhanced_utg = dynamic_graph(json_path=enhance_utg_path)
        summary = summary + 1

        unique_data = getAdStatus(apk_path)
        if unique_data:
            ad_count += 1

            type2 = check_type2(enhanced_utg)
            type3 = check_type3(enhanced_utg)
            type4 = check_type4(enhanced_utg)
            type5 = check_type5(enhanced_utg)
            type6 = check_type6(enhanced_utg)

            if type2 != []:
                type2_list.append(type2)
            
            if type3 != []:
                type3_list.append(type3)

            if type4 != []:
                type4_list.append(type4)

            if type5 != []:
                type5_list.append(type5)

            if type5 != []:
                type6_list.append(type6)

    print("type2: ", str(len(type2_list)))
    print("type3: ", str(len(type3_list)))
    print("type4: ", str(len(type4_list)))
    print("type5: ", str(len(type5_list)))
    print("type6: ", str(len(type6_list)))
    print("summary: ", str(summary))
    print("failed count: ", str(utg_fail_count))

    paths_json = os.path.join(path, "path.json")
    results = extract_paths_to_ads(enhanced_utg, None, 20, paths_json)
    

    has_ad = False
    has_frida_logs = False

    #rets = check_type2(enhanced_utg)
    # for ret in rets:
    #     f"{ret['src_node']} -> {ret['event_type']} -> {ret['dst_ad_node']}"
    
    # ret3 = check_type3(enhanced_utg)
    # ret = check_type5(enhanced_utg)


    # check if the ad_states.json exist and store the ad state
    # data = getAdStates(path, enhance_utg_path, results)
    # if data:
    #     has_ad = True
        # print("Has ads")
        # get the list of ad states
        # print(data)


    # check if the frida_log.json exist.

    # stats for app with ads

    pass
"""

def analyze(path, output_csv="apk_analysis_results.csv"):
    """
    åˆ†ææŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰APKï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ°CSVæ–‡ä»¶ä¸­
    
    Args:
        path: åŒ…å«APKåˆ†æç»“æœçš„æ ¹ç›®å½•
        output_csv: è¾“å‡ºCSVæ–‡ä»¶çš„è·¯å¾„
    """
    print(f"[+] Starting analysis of directory: {path}")
    
    # å‡†å¤‡CSVæ–‡ä»¶
    fieldnames = [
        "app_name", 
        "has_ad", 
        "type2_detected", "type2_features",
        "type3_detected", "type3_features", 
        "type4_detected", "type4_features",
        "type5_detected", "type5_features", 
        "type6_detected", "type6_features",
        "analysis_date"
    ]
    
    # åˆ›å»ºæˆ–æ¸…ç©ºCSVæ–‡ä»¶å¹¶å†™å…¥è¡¨å¤´
    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
    
    # è·å–æ‰€æœ‰APKç›®å½•
    folders = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))]
    print(f"[+] Found {len(folders)} APK directories")
    
    # åˆå§‹åŒ–ç»Ÿè®¡è®¡æ•°å™¨
    stats = {
        "total_apks": len(folders),
        "apks_with_ads": 0,
        "type2_count": 0,
        "type3_count": 0,
        "type4_count": 0,
        "type5_count": 0,
        "type6_count": 0,
        "failed_analysis": 0
    }
    
    # åˆ†ææ¯ä¸ªAPK
    for app_dir in folders:
        apk_path = os.path.join(path, app_dir)
        result = analyze_single_apk(apk_path, app_dir)
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        if result:
            if result["has_ad"]:
                stats["apks_with_ads"] += 1
            
            if result["type2_detected"]:
                stats["type2_count"] += 1
                
            if result["type3_detected"]:
                stats["type3_count"] += 1
                
            if result["type4_detected"]:
                stats["type4_count"] += 1
                
            if result["type5_detected"]:
                stats["type5_count"] += 1
                
            if result["type6_detected"]:
                stats["type6_count"] += 1
            
            # å°†ç»“æœå†™å…¥CSV
            with open(output_csv, 'a', newline='', encoding='utf-8') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writerow(result)
            
            print(f"[+] Results for {app_dir} saved to {output_csv}")
        else:
            stats["failed_analysis"] += 1
    
    # ç”Ÿæˆå¹¶è¾“å‡ºæ€»ä½“ç»Ÿè®¡æŠ¥å‘Š
    generate_summary_report(stats, output_csv)
    
    return stats
    
def generate_summary_report(stats, output_csv):
    """
    ç”Ÿæˆå¹¶è¾“å‡ºæ€»ä½“ç»Ÿè®¡æŠ¥å‘Š
    
    Args:
        stats: åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        output_csv: è¾“å‡ºCSVæ–‡ä»¶çš„è·¯å¾„
    """
    # åˆ›å»ºæŠ¥å‘Šæ–‡ä»¶å
    report_file = output_csv.replace(".csv", "_summary.txt")
    
    # è®¡ç®—ç™¾åˆ†æ¯”
    if stats["total_apks"] > 0:
        ad_percentage = (stats["apks_with_ads"] / stats["total_apks"]) * 100
        type2_percentage = (stats["type2_count"] / stats["apks_with_ads"]) * 100 if stats["apks_with_ads"] > 0 else 0
        type3_percentage = (stats["type3_count"] / stats["apks_with_ads"]) * 100 if stats["apks_with_ads"] > 0 else 0
        type4_percentage = (stats["type4_count"] / stats["apks_with_ads"]) * 100 if stats["apks_with_ads"] > 0 else 0
        type5_percentage = (stats["type5_count"] / stats["apks_with_ads"]) * 100 if stats["apks_with_ads"] > 0 else 0
        type6_percentage = (stats["type6_count"] / stats["apks_with_ads"]) * 100 if stats["apks_with_ads"] > 0 else 0
    else:
        ad_percentage = type2_percentage = type3_percentage = type4_percentage = type5_percentage = type6_percentage = 0
    
    # åˆ›å»ºæŠ¥å‘Šå†…å®¹
    report_content = f"""
    APK åˆ†ææ€»ä½“ç»Ÿè®¡æŠ¥å‘Š
    ç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
    ==================================================

    æ€»ä½“ç»Ÿè®¡:
    - åˆ†æçš„APKæ€»æ•°: {stats["total_apks"]}
    - åŒ…å«å¹¿å‘Šçš„APKæ•°é‡: {stats["apks_with_ads"]} ({ad_percentage:.2f}%)
    - åˆ†æå¤±è´¥çš„APKæ•°é‡: {stats["failed_analysis"]}

    å¹¿å‘Šç±»å‹æ£€æµ‹ç»Ÿè®¡:
    - æ£€æµ‹åˆ°Type2çš„APKæ•°é‡: {stats["type2_count"]} ({type2_percentage:.2f}% of ad-containing APKs)
    - æ£€æµ‹åˆ°Type3çš„APKæ•°é‡: {stats["type3_count"]} ({type3_percentage:.2f}% of ad-containing APKs)
    - æ£€æµ‹åˆ°Type4çš„APKæ•°é‡: {stats["type4_count"]} ({type4_percentage:.2f}% of ad-containing APKs)
    - æ£€æµ‹åˆ°Type5çš„APKæ•°é‡: {stats["type5_count"]} ({type5_percentage:.2f}% of ad-containing APKs)
    - æ£€æµ‹åˆ°Type6çš„APKæ•°é‡: {stats["type6_count"]} ({type6_percentage:.2f}% of ad-containing APKs)

    è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {output_csv}
    """
    
    # å†™å…¥æŠ¥å‘Šæ–‡ä»¶
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    # æ‰“å°æŠ¥å‘Šåˆ°æ§åˆ¶å°
    print(report_content)
    
    # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
    #generate_visualizations(stats, output_csv)
    
    return report_content

def analyze_single_apk(apk_path, app_dir):
    """
    åˆ†æå•ä¸ªAPKç›®å½•
    
    Args:
        apk_path: APKç›®å½•çš„å®Œæ•´è·¯å¾„
        app_dir: APKç›®å½•åç§°ï¼ˆç”¨ä½œapp_nameï¼‰
    
    Returns:
        dict: åŒ…å«åˆ†æç»“æœçš„å­—å…¸
    """
    print(f"[+] Analyzing APK: {app_dir}")
    app_info = extract_app_info(apk_path, app_dir)
    if not app_info:
        app_info = {
            'app_name': app_dir,
            'app_path': apk_path,
            'utg_exists': 'TRUE',
            'test_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'is_tested': 'TRUE'
        }


    # åˆå§‹åŒ–ç»“æœå­—å…¸
    result = {
        "app_name": app_dir,
        "has_ad": False,
        "type2_detected": False, "type2_features": "",
        "type3_detected": False, "type3_features": "",
        "type4_detected": False, "type4_features": "",
        "type5_detected": False, "type5_features": "",
        "type6_detected": False, "type6_features": "",
        "analysis_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }
    
    # æ£€æŸ¥UTGæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    utg_path = os.path.join(apk_path, "utg.js")
    if not os.path.exists(utg_path):
        print(f"[!] utg.js does not exist: {utg_path}")
        # ä»ç„¶æ›´æ–°å…¨å±€æ–‡ä»¶ï¼Œæ ‡è®°ä¸ºå·²æµ‹è¯•ä½†æ— UTG
        app_info['utg_exists'] = 'FALSE'
        update_global_files(app_info, result)
        return result
    
    # å¢å¼ºUTG
    try:
        utg = dynamic_graph(js_path=utg_path)
        utg.enhance_utg(apk_path)
        enhance_utg_path = os.path.join(apk_path, "enhanced_utg.json")
        enhanced_utg = dynamic_graph(json_path=enhance_utg_path)
    except Exception as e:
        print(f"[!] Failed to enhance UTG for {app_dir}: {e}")
        # æ›´æ–°å…¨å±€æ–‡ä»¶ï¼Œæ ‡è®°åˆ†æå¤±è´¥
        update_global_files(app_info, result)
        return result
    
    # æ£€æŸ¥å¹¿å‘ŠçŠ¶æ€
    unique_data = getAdStatus(apk_path)
    if unique_data:
        result["has_ad"] = True
        
        # æ£€æŸ¥å„ç§ç±»å‹
        type2_results = check_type2(enhanced_utg)
        type3_results = check_type3(enhanced_utg)
        type4_results = check_type4(enhanced_utg)
        type5_results = check_type5(enhanced_utg)
        type6_results = check_type6(enhanced_utg)
        
        # æ›´æ–°ç»“æœ
        if type2_results:
            result["type2_detected"] = True
            result["type2_features"] = json.dumps(type2_results, ensure_ascii=False)
        
        if type3_results:
            result["type3_detected"] = True
            result["type3_features"] = json.dumps(type3_results, ensure_ascii=False)
        
        if type4_results:
            result["type4_detected"] = True
            result["type4_features"] = json.dumps(type4_results, ensure_ascii=False)
        
        if type5_results:
            result["type5_detected"] = True
            result["type5_features"] = json.dumps(type5_results, ensure_ascii=False)
        
        if type6_results:
            result["type6_detected"] = True
            result["type6_features"] = json.dumps(type6_results, ensure_ascii=False)
    
    # æ›´æ–°å…¨å±€æ–‡ä»¶
    update_global_files(app_info, result)

    return result

# single test
def analyze_test(path):

    results = {
        'total_apk': 0,
        'apks_with_ad_states': 0,
        'apks_with_frida_logs': 0,
        'ad_state_counts': 0,

    }

    result = {
        "package_name": None,
        "sha": None,
        "has_ad": False
    }

    utg_path = os.path.join(path, "utg.js")
    utg = dynamic_graph(js_path=utg_path)
    utg.enhance_utg(path)

    enhance_utg_path = os.path.join(path, "enhanced_utg.json")
    enhanced_utg = dynamic_graph(json_path=enhance_utg_path)

    has_ad = False
    has_frida_logs = False

    # result_list = extract_paths_to_ads(enhanced_utg)
    paths_json = os.path.join(path, "path.json")
    results = extract_paths_to_ads(enhanced_utg, None, 20, paths_json)
    print(results)

    rets = check_type2(enhanced_utg)
    for ret in rets:
        f"{ret['src_node']} -> {ret['event_type']} -> {ret['dst_ad_node']}"
    
    rets3 = check_type3(enhanced_utg)
    ret = check_type5(enhanced_utg)


    # check if the ad_states.json exist and store the ad state
    # data = getAdStates(path, enhance_utg_path, results)
    # if data:
    #     has_ad = True
        # print("Has ads")
        # get the list of ad states
        # print(data)


    # check if the frida_log.json exist.

    # stats for app with ads

    pass

def getAdStatus(app_path):
    # unique_data = None
    # print(f"[+] Start analyze ad states " + app_path)

    # ad_state = False
    # if not os.path.isdir(path):
    #     print(f"Error! The dir path is not exist -- {path}.")
    #     return None

    # for file_name in os.listdir(app_path):
    #     if file_name.endswith("ad_states.json"):
    #         ad_state_path = os.path.join(app_path, file_name)
    #         unique_data = get_unique_ad_states(ad_state_path)
    #         break
    
    # return unique_data
    """æ£€æŸ¥APKæ˜¯å¦åŒ…å«å¹¿å‘Š"""
    unique_data = None
    print(f"[+] Checking ad status for: {app_path}")

    if not os.path.isdir(app_path):
        print(f"Error! The dir path does not exist: {app_path}.")
        return None

    for file_name in os.listdir(app_path):
        if file_name.endswith("ad_states.json"):
            ad_state_path = os.path.join(app_path, file_name)
            unique_data = get_unique_ad_states(ad_state_path)
            break
    
    return unique_data

def getAdStatics(path, enhanced_utg_path, results):
    print("[+] start analyzing ad states")
    ad_states_path = None
    if not os.path.isdir(path):
        print(f"Error! The dir path is not exist -- {path}.")
        return None
    
    for apk_folder in os.listdir(path):
        apk_path = os.path.join(path, apk_folder)

        if not os.path.isdir(apk_path):
            continue
        
        results['total_apk'] += 1
        
        for file_name in os.listdir(path):
            #print(file_name)
            if file_name.endswith("ad_states.json"):
                # check if the ad_states.json exist and store the ad state
                ad_states_path = os.path.join(path, file_name)
                break
            
        unique_data = []
        seen_items = set() 
            
        if ad_states_path and os.path.isfile(ad_states_path):
            with open(ad_states_path, "r", encoding="utf-8") as f:
                try:
                    data = json.load(f)
                    for item in data:
                        state_id = item.get("state_str")
                        screenshot_path = item.get("screenshot_path")
                        if state_id not in seen_items and path in screenshot_path:
                            seen_items.add(state_id)
                            unique_data.append(item)
                except json.JSONDecodeError:
                    print("JSON format error: ", ad_states_path)
            
            # å­˜å›å»ï¼ˆè¦†ç›–åŸæ–‡ä»¶ï¼‰
            # with open(ad_states_path, "w", encoding="utf-8") as f:
            #     json.dump(unique_data, f, indent=2, ensure_ascii=False)
            # ä¿å­˜ä¸ºå‰¯æœ¬
            dedup_path = ad_states_path.replace(".json", "_dedup.json")
            with open(dedup_path, "w", encoding="utf-8") as f:
                json.dump(unique_data, f, indent=2, ensure_ascii=False)

            print(f"å»é‡åä¿å­˜æˆåŠŸï¼Œå…± {len(unique_data)} æ¡è®°å½•ã€‚")
        else:
            print("æœªæ‰¾åˆ° ad_states.json æ–‡ä»¶")
            return None

    return unique_data

# å…¨å±€å˜é‡å®šä¹‰
GLOBAL_MASTER_CSV = "all_apps_master.csv"
GLOBAL_CHECKED_TXT = "checked_apps.txt"

def ensure_global_files():
    """ç¡®ä¿å…¨å±€CSVå’ŒTXTæ–‡ä»¶å­˜åœ¨"""
    # ç¡®ä¿CSVæ–‡ä»¶å­˜åœ¨ä¸”æœ‰æ­£ç¡®çš„åˆ—
    csv_columns = [
        'app_name', 'app_path', 'package_name', 'apk_path', 'sha256', 
        'is_tested', 'test_date', 'utg_exists', 'app_output_dir',
        'year', 'size', 'contain_ad', 'sensor_test_done', 'timestamp',
        'has_ad', 'type2_detected', 'type3_detected', 'type4_detected',
        'type5_detected', 'type6_detected', 'analysis_date'
    ]
    
    if not os.path.exists(GLOBAL_MASTER_CSV):
        df = pd.DataFrame(columns=csv_columns)
        df.to_csv(GLOBAL_MASTER_CSV, index=False)
        print(f"[+] åˆ›å»ºå…¨å±€ä¸»CSVæ–‡ä»¶: {GLOBAL_MASTER_CSV}")
    
    # ç¡®ä¿TXTæ–‡ä»¶å­˜åœ¨
    if not os.path.exists(GLOBAL_CHECKED_TXT):
        with open(GLOBAL_CHECKED_TXT, 'w', encoding='utf-8') as f:
            f.write("# å·²æ£€æµ‹åº”ç”¨åˆ—è¡¨\n")
        print(f"[+] åˆ›å»ºå…¨å±€TXTæ–‡ä»¶: {GLOBAL_CHECKED_TXT}")

def load_checked_apps():
    """åŠ è½½å·²æ£€æŸ¥çš„åº”ç”¨åˆ—è¡¨"""
    checked_apps = set()
    
    if os.path.exists(GLOBAL_CHECKED_TXT) and os.path.getsize(GLOBAL_CHECKED_TXT) > 0:
        with open(GLOBAL_CHECKED_TXT, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    checked_apps.add(line)
    
    return checked_apps

def update_global_files(app_info, analysis_result=None):
    """
    æ›´æ–°å…¨å±€CSVå’ŒTXTæ–‡ä»¶
    
    Args:
        app_info: åº”ç”¨åŸºæœ¬ä¿¡æ¯å­—å…¸
        analysis_result: åˆ†æç»“æœå­—å…¸ï¼ˆå¯é€‰ï¼‰
    """
    try:
        # è¯»å–ç°æœ‰CSV
        if os.path.exists(GLOBAL_MASTER_CSV) and os.path.getsize(GLOBAL_MASTER_CSV) > 0:
            master_df = pd.read_csv(GLOBAL_MASTER_CSV, dtype=str).fillna("")
        else:
            master_df = pd.DataFrame(columns=[
                'app_name', 'app_path', 'package_name', 'apk_path', 'sha256', 
                'is_tested', 'test_date', 'utg_exists', 'app_output_dir',
                'year', 'size', 'contain_ad', 'sensor_test_done', 'timestamp',
                'has_ad', 'type2_detected', 'type3_detected', 'type4_detected',
                'type5_detected', 'type6_detected', 'analysis_date'
            ])
        
        # è¯»å–ç°æœ‰TXT
        checked_apps = load_checked_apps()
        
        app_name = app_info.get('app_name', '')
        if not app_name:
            return False
        
        # åˆå¹¶åº”ç”¨ä¿¡æ¯å’Œåˆ†æç»“æœ
        merged_info = app_info.copy()
        if analysis_result:
            merged_info.update(analysis_result)
        
        # è®¾ç½®é»˜è®¤å€¼
        merged_info.setdefault('is_tested', 'TRUE')
        merged_info.setdefault('test_date', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
        merged_info.setdefault('utg_exists', 'TRUE')
        merged_info.setdefault('analysis_date', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        existing_mask = master_df['app_name'] == app_name
        
        if existing_mask.any():
            # æ›´æ–°ç°æœ‰è®°å½•
            for idx in master_df[existing_mask].index:
                for col, value in merged_info.items():
                    if col in master_df.columns and value:
                        master_df.loc[idx, col] = value
            print(f"[+] æ›´æ–°å…¨å±€è®°å½•: {app_name}")
        else:
            # æ·»åŠ æ–°è®°å½•
            new_row = {col: '' for col in master_df.columns}
            new_row.update(merged_info)
            master_df = pd.concat([master_df, pd.DataFrame([new_row])], ignore_index=True)
            checked_apps.add(app_name)
            print(f"[+] æ·»åŠ å…¨å±€è®°å½•: {app_name}")
        
        # ä¿å­˜æ–‡ä»¶
        master_df.to_csv(GLOBAL_MASTER_CSV, index=False)
        
        with open(GLOBAL_CHECKED_TXT, 'w', encoding='utf-8') as f:
            f.write("# å·²æ£€æµ‹åº”ç”¨åˆ—è¡¨\n")
            for app_name in sorted(checked_apps):
                f.write(f"{app_name}\n")
        
        return True
        
    except Exception as e:
        print(f"[!] æ›´æ–°å…¨å±€æ–‡ä»¶å¤±è´¥: {e}")
        return False

def scan_and_update_multiple_folders(folder_paths, recursive=True):
    """
    æ‰«æå¤šä¸ªæ–‡ä»¶å¤¹ï¼Œå°†æ–°çš„åº”ç”¨æ·»åŠ åˆ°å…¨å±€æ–‡ä»¶
    
    Args:
        folder_paths: æ–‡ä»¶å¤¹è·¯å¾„åˆ—è¡¨
        recursive: æ˜¯å¦é€’å½’æœç´¢å­æ–‡ä»¶å¤¹
        
    Returns:
        tuple: (æ–°å‘ç°çš„åº”ç”¨æ•°é‡, æ€»åº”ç”¨æ•°é‡)
    """
    try:
        # ç¡®ä¿å…¨å±€æ–‡ä»¶å­˜åœ¨
        ensure_global_files()
        
        # è¯»å–å·²æ£€æŸ¥çš„åº”ç”¨åˆ—è¡¨
        checked_apps = load_checked_apps()
        
        new_apps_count = 0
        total_apps_count = 0
        
        for folder_path in folder_paths:
            if not os.path.exists(folder_path):
                print(f"[!] æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
                continue
            
            print(f"[+] æ‰«ææ–‡ä»¶å¤¹: {folder_path}")
            
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„åº”ç”¨ç»“æœæ–‡ä»¶å¤¹
            app_folders = find_app_folders(folder_path, recursive)
            print(f"    æ‰¾åˆ° {len(app_folders)} ä¸ªå¯èƒ½çš„åº”ç”¨æ–‡ä»¶å¤¹")
            
            for app_folder in app_folders:
                total_apps_count += 1
                app_name = os.path.basename(app_folder.rstrip(os.sep))
                
                # æ£€æŸ¥æ˜¯å¦å·²è®°å½•
                if app_name in checked_apps:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨utg.js
                utg_path = os.path.join(app_folder, "utg.js")
                if not os.path.exists(utg_path):
                    continue
                
                # æå–åº”ç”¨ä¿¡æ¯
                app_info = extract_app_info(app_folder, app_name)
                if app_info:
                    # æ›´æ–°å…¨å±€æ–‡ä»¶
                    if update_global_files(app_info):
                        new_apps_count += 1
                        print(f"[+] å‘ç°å¹¶è®°å½•æ–°åº”ç”¨: {app_name}")
        
        print(f"\n[+] æ‰«æå®Œæˆ:")
        print(f"    - æ€»åº”ç”¨æ–‡ä»¶å¤¹: {total_apps_count}")
        print(f"    - æ–°å‘ç°åº”ç”¨: {new_apps_count}")
        print(f"    - å…¨å±€CSV: {GLOBAL_MASTER_CSV}")
        print(f"    - å…¨å±€TXT: {GLOBAL_CHECKED_TXT}")
        
        return new_apps_count, total_apps_count
        
    except Exception as e:
        print(f"[!] æ‰«ææ–‡ä»¶å¤¹å¤±è´¥: {e}")
        return 0, 0

def find_app_folders(root_path, recursive=True):
    """æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„åº”ç”¨æ–‡ä»¶å¤¹"""
    app_folders = []
    
    if recursive:
        # é€’å½’æœç´¢æ‰€æœ‰å­æ–‡ä»¶å¤¹
        for root, dirs, files in os.walk(root_path):
            # æ£€æŸ¥å½“å‰ç›®å½•æ˜¯å¦åŒ…å«utg.js
            if "utg.js" in files:
                app_folders.append(root)
    else:
        # åªæœç´¢ç›´æ¥å­æ–‡ä»¶å¤¹
        for item in os.listdir(root_path):
            item_path = os.path.join(root_path, item)
            if os.path.isdir(item_path):
                utg_path = os.path.join(item_path, "utg.js")
                if os.path.exists(utg_path):
                    app_folders.append(item_path)
    
    return app_folders

def extract_app_info(app_folder, app_name):
    """ä»åº”ç”¨æ–‡ä»¶å¤¹ä¸­æå–ä¿¡æ¯"""
    try:
        app_info = {
            'app_name': app_name,
            'app_path': app_folder,
            'utg_exists': 'TRUE',
            'test_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'is_tested': 'TRUE'
        }
        
        # å°è¯•ä»utg.jsä¸­æå–æ›´å¤šä¿¡æ¯
        utg_path = os.path.join(app_folder, "utg.js")
        if os.path.exists(utg_path):
            try:
                with open(utg_path, 'r', encoding='utf-8') as f:
                    utg_content = f.read()
                    # å°è¯•è§£æJSONï¼ˆutg.jsé€šå¸¸æ˜¯JSONæ ¼å¼ï¼‰
                    if utg_content.strip().startswith('{'):
                        utg_data = json.loads(utg_content)
                        package_name = utg_data.get('packageName', '')
                        if package_name:
                            app_info['package_name'] = package_name
            except:
                # å¦‚æœè§£æå¤±è´¥ï¼Œè·³è¿‡
                pass
        
        # å°è¯•æŸ¥æ‰¾APKæ–‡ä»¶
        import glob
        apk_files = glob.glob(os.path.join(app_folder, "*.apk"))
        if apk_files:
            app_info['apk_path'] = apk_files[0]
        
        # å°è¯•è¯»å–å…¶ä»–å¯èƒ½å­˜åœ¨çš„å…ƒæ•°æ®æ–‡ä»¶
        meta_files = ['app_info.json', 'metadata.json', 'analysis_result.json']
        for meta_file in meta_files:
            meta_path = os.path.join(app_folder, meta_file)
            if os.path.exists(meta_path):
                try:
                    with open(meta_path, 'r', encoding='utf-8') as f:
                        meta_data = json.load(f)
                        # æå–æœ‰ç”¨çš„å­—æ®µ
                        for key in ['sha256', 'year', 'size', 'contain_ad', 'sensor_test_done', 'timestamp']:
                            if key in meta_data:
                                app_info[key] = str(meta_data[key])
                except:
                    pass
        
        return app_info
        
    except Exception as e:
        print(f"[!] æå–åº”ç”¨ä¿¡æ¯å¤±è´¥ {app_folder}: {e}")
        return None

def get_global_stats():
    """è·å–å…¨å±€æ–‡ä»¶çš„ç»Ÿè®¡ä¿¡æ¯"""
    try:
        if not os.path.exists(GLOBAL_MASTER_CSV) or os.path.getsize(GLOBAL_MASTER_CSV) == 0:
            return {"total_apps": 0, "tested_apps": 0}
        
        df = pd.read_csv(GLOBAL_MASTER_CSV, dtype=str).fillna("")
        
        total_apps = len(df)
        tested_apps = len(df[df['is_tested'].str.upper().isin(['TRUE', 'T', '1', 'YES', 'Y'])])
        utg_exists = len(df[df['utg_exists'].str.upper().isin(['TRUE', 'T', '1', 'YES', 'Y'])])
        has_ad = len(df[df['has_ad'].str.upper().isin(['TRUE', 'T', '1', 'YES', 'Y'])])
        
        stats = {
            "total_apps": total_apps,
            "tested_apps": tested_apps,
            "utg_exists": utg_exists,
            "has_ad": has_ad,
            "tested_percentage": (tested_apps / total_apps * 100) if total_apps > 0 else 0
        }
        
        return stats
        
    except Exception as e:
        print(f"[!] è·å–å…¨å±€ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        return {"total_apps": 0, "tested_apps": 0}

def print_global_stats():
    """æ‰“å°å…¨å±€ç»Ÿè®¡ä¿¡æ¯"""
    stats = get_global_stats()
    
    print(f"\nğŸ“Š å…¨å±€æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   æ€»åº”ç”¨æ•°: {stats['total_apps']}")
    print(f"   å·²æµ‹è¯•åº”ç”¨: {stats['tested_apps']}")
    print(f"   æœ‰UTGæ–‡ä»¶: {stats['utg_exists']}")
    print(f"   åŒ…å«å¹¿å‘Š: {stats['has_ad']}")
    print(f"   æµ‹è¯•å®Œæˆç‡: {stats['tested_percentage']:.1f}%")
    print(f"   å…¨å±€CSV: {GLOBAL_MASTER_CSV}")
    print(f"   å…¨å±€TXT: {GLOBAL_CHECKED_TXT}")

# def get_unique_ad_states(path):
#     print("[+] get unique ad status: " + path)
#     unique_data = []
#     seen_items = set() 
            
#     if path and os.path.isfile(path):
#         with open(path, "r", encoding="utf-8") as f:
#             try:
#                 data = json.load(f)
#                 if data is None:
#                     return None
                
#                 for item in data:
                    
#                     state_id = item.get("state_str")
#                     screenshot_path = item.get("screenshot_path")
#                     if state_id not in seen_items:
#                         seen_items.add(state_id)
#                         unique_data.append(item)
#             except json.JSONDecodeError:
#                 print("JSON format error: ", path)
#         print("unique_data: ", unique_data)
#         # å­˜å›å»ï¼ˆè¦†ç›–åŸæ–‡ä»¶ï¼‰
#         # with open(ad_states_path, "w", encoding="utf-8") as f:
#         #     json.dump(unique_data, f, indent=2, ensure_ascii=False)
#         # ä¿å­˜ä¸ºå‰¯æœ¬
#         dedup_path = path.replace(".json", "_dedup.json")
#         with open(dedup_path, "w", encoding="utf-8") as f:
#             json.dump(unique_data, f, indent=2, ensure_ascii=False)

#         print(f"å»é‡åä¿å­˜æˆåŠŸï¼Œå…± {len(unique_data)} æ¡è®°å½•ã€‚")
#     else:
#         print("æœªæ‰¾åˆ° ad_states.json æ–‡ä»¶")
#         return None

#     return unique_data

def get_unique_ad_states(path):
    """è·å–å”¯ä¸€çš„å¹¿å‘ŠçŠ¶æ€"""
    print(f"[+] Getting unique ad states: {path}")
    unique_data = []
    seen_items = set()
    
    if path and os.path.isfile(path):
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
                if data is None:
                    return None
                
                for item in data:
                    state_id = item.get("state_str")
                    if state_id and state_id not in seen_items:
                        seen_items.add(state_id)
                        unique_data.append(item)
        except json.JSONDecodeError:
            print(f"JSON format error: {path}")
            return None
        
        # ä¿å­˜å»é‡åçš„å‰¯æœ¬
        dedup_path = path.replace(".json", "_dedup.json")
        try:
            with open(dedup_path, "w", encoding="utf-8") as f:
                json.dump(unique_data, f, indent=2, ensure_ascii=False)
            print(f"Saved deduplicated ad states to {dedup_path}, total records: {len(unique_data)}")
        except Exception as e:
            print(f"Failed to save deduplicated ad states: {e}")
    else:
        print("ad_states.json file not found")
        return None

    return unique_data

def detect_misleading_UI(image_path):
    pass

def check_type2(graph):
    """
        éå†æ‰€æœ‰ ad-related çŠ¶æ€ï¼Œæ£€æŸ¥å…¶æ¥æº srcï¼š
        - å¦‚æœ src ä¸æ˜¯ ad-related / æ˜¯ad-relatedä½†æ˜¯banner format
        - ä¸” event ä¸æ˜¯ touchad_event
        - ä¸”è¾¹çš„ç±»å‹ä¸æ˜¯ adcomposite
        åˆ™è®¤ä¸ºæ˜¯å¯ç–‘çš„åŠŸèƒ½æ€§ä¸­æ–­
        """
    print("[+] Checking type2 ...")
    results = []

    for node_id, node in graph.state.items():
        
        if not node.get("is_ad_related", False):
            continue
        
        for src, edge_info in node.get("src", {}).items():
            src_node = graph.state[src]

            if src_node.get("is_ad_related", False) and src_node.get("ad_format") != "banner":
                continue
            
            for e in edge_info.get("events", []):
                etype = e.get("event_type", "").lower()

                detail = e.get("event_str").lower()
                if etype == "key" and "back" in detail:
                    etype = "key_back"

                if "adcomposite" in etype:
                    continue

                # å¦‚æœä¸æ˜¯ touchad_eventï¼Œåˆ™è®°å½•
                dst_activity = node.get("activity", "unknown")
                src_activity = src_node.get("activity", "unknown")
                if "touch_ad" not in etype:

                    if src_node.get("ad_format") == "banner":
                        results.append({
                            "dst_ad_node": node_id,
                            "src_node": src,
                            #"activity": node.get("activity"),
                            "dst_activity": node.get("activity", "unknown"),
                            "src_activity": src_node.get("activity", "unknown"),
                            "event_type": etype,
                            "edge_info": e,
                            "pattern": f"{src}(banner ad) --[{etype}]--> {node_id}(ad)"
                        })
                        print(f"  Found: {src}(banner ad){src_activity} --[{etype}]--> {node_id}(ad){dst_activity}")
                    else:
                        results.append({
                            "dst_ad_node": node_id,
                            "src_node": src,
                            #"activity": node.get("activity"),
                            "dst_activity": node.get("activity", "unknown"),
                            "src_activity": src_node.get("activity", "unknown"),
                            "event_type": etype,
                            "edge_info": e,
                            "pattern": f"{src}(non-ad) --[{etype}]--> {node_id}(ad)"
                        })

                        # æ‰“å°ç®€æ´çš„è¾“å‡ºæ ¼å¼
                        print(f"  Found: {src}(non-ad){src_activity} --[{etype}]--> {node_id}(ad){dst_activity}")
    return results

def check_type3(graph):
    """
        ä¸¤ç§æƒ…å†µï¼š
        1. ad-related èŠ‚ç‚¹ -> æŒ‰ back -> ä»ç„¶ ad-related (åç»­å¯åšé¡µé¢ç›¸ä¼¼åº¦æ£€æµ‹)
        2. ad-related èŠ‚ç‚¹ï¼Œä¸” is_external = True -> back -> ä»ç„¶ external
        """
    
    print("[+] Checking type3")
    results = []
    for node_id, node in graph.state.items():
        
        if not node.get("is_ad_related", False):
            continue
        
        # éå†srcè¾¹ï¼Œå³è¿›å…¥è¿™ä¸ªå¹¿å‘Šçš„æ¥æº
        for src, edge_info in node.get("src", {}).items():
            # if src == "50b318375205e8450a077c6b68cfb85e":
            #     print("11111+", node_id)
            #     print(edge_info.get("is_ad_related", False))
            #     print(node.get("is_ad_related", False))
            #     print(edge_info)
            for e in edge_info.get("events", []):
                
                etype = e.get("event_type", "").lower()
                detail = e.get("event_str").lower()
                if etype == "key" and "back" in detail:
                    etype = "key_back"
                # if src == "cd2bdebcfacf84ca65b85335dd96d131":
                #     print(etype)

                if etype == "key_back": # no banner
                    # ignore case that external_site (ad-content) back to aad
                    if edge_info.get("is_external_site", False):
                        continue
                    # if src == "cd2bdebcfacf84ca65b85335dd96d131":
                    #     print(etype)
                    # cd2bdebcfacf84ca65b85335dd96d131-->72832348da8e84a26b234ef8a63fc5a7
                    if edge_info.get("is_ad_related", False):
                        
                        ret = {
                            "node": node_id,
                            "activity": node.get("activity"),
                            "case": "back_still_ad",
                            "event": etype,
                            "edge_info": e,
                            "pattern": f"{src}(ad-related) --[{etype}]--> {node_id}(ad-related)"
                        }
                        if ret not in results:
                            results.append(ret)
                        print(f"  Found: {src}(ad) --[{etype}]--> {node_id}(ad)")
                    
                    # æƒ…å†µ2: external èŠ‚ç‚¹ back ä»ç„¶ external
                    if node.get("is_external", False) and edge_info.get("is_external", False):
                        print("both external")
                        results.append({
                            "node": node_id,
                            "activity": node.get("activity"),
                            "case": "back_still_external",
                            "event": etype,
                            "edge_info": e,
                                "pattern": f"{src}(is_external) --[{etype}]--> {node_id}(ad)"
                        })
                        print(f"  Found: {src}(is_external) --[{etype}]--> {node_id}(is_external)")
        return results


        # src_nodes = node.get("src", {})
        # #if src_nodes.items():
        # for src_id, edge_info in src_nodes.items():
        #     src_node = graph.state.get(src_id, {})
        #     #for src_node in src_nodes:

        #     if not src_node.get("is_ad_related", False):
        #         events = edge_info.get("events", [])
        #         for e in events:
        #             if e.get("event_type") != "touch_ad":
        #                 print(f"[Type2] Ad node {node_id} "
        #                   f"triggered from non-ad node {src_id} "
        #                   f"via event {e}")
        #         # print(src_node)
        #         # continue
    
def check_type4(graph):

    """
    4. aggressive redirection: state A (is_ad_related) -> wait -> state B (is_external)
    """

    results = []

    for node_id, node in graph.state.items():
        
        if not node.get("is_ad_related", False):
            continue
        
        # éå†srcè¾¹ï¼Œå³è¿›å…¥è¿™ä¸ªå¹¿å‘Šçš„æ¥æº
        for src, edge_info in node.get("src", {}).items():
            
            for e in edge_info.get("events", []):
                etype = e.get("event_type", "").lower()
                detail = e.get("event_str").lower()
                if etype == "wait":
                    # ignore the non is_external node
                    if edge_info.get("is_external_site", False):
                        continue
                    
                    results.append({
                            "node": node_id,
                            "activity": node.get("activity"),
                            "case": "type4",
                            "event": etype,
                            "edge_info": e,
                            "pattern": f"{src}(ad) --[{etype}]--> {node_id}(external)"
                        })
                    print(f"  Found: {src}(ad) --[{etype}]--> {node_id}(external)")
        return results

def check_type5(graph):
    """
    5. Outside-App Ads: - UI activities do not belong to the app
    """
    print("[+] checking type5 outside-app ads ...")
    results = []
    for node_id, node in graph.state.items():
        # not to resolve the non-ad-related nodes
        # if there is no "is_ad_related", return the False
        if not node.get("is_ad_related", False):
            continue
        
        if "Browser" in node.get("activity", ""):
            continue
        
        #if "launcher" in node.get("package") or node.get("package").startswith("com.android"):
        if "launcher" in node.get("package", "") or node.get("package", "").startswith("com.android"):    
            results.append({
                "node": node_id,
                "activity": node.get("activity"),
                "case": "outside-app ads",
                "pattern": f"{node_id}(ad) -- out of app"
            })
            print("[+] {node_id}(ad) -- out of app", node_id)

    return results

def check_type6(graph, ad_ratio_threshold=0.3, consecutive_threshold=3, gap_threshold=2):
    """
    6. Frequency
    """

    print("[+] checking type6 frequency")
    results = []

    ad_nodes = [nid for nid, n in graph.state.items() if n.get("is_ad_related", False)]
    total_nodes = len(graph.state)
    ad_count = len(ad_nodes)

    if total_nodes == 0:
        return results

    # è§„åˆ™1: å¹¿å‘Šæ¯”ä¾‹è¿‡é«˜
    ad_ratio = ad_count / total_nodes
    if ad_ratio > ad_ratio_threshold:
        ret = {
            "case": "high_ad_ratio",
            "ad_ratio": ad_ratio,
            "ad_count": ad_count,
            "total_nodes": total_nodes,
            "pattern": f"Ad ratio {ad_ratio:.2f} exceeds threshold {ad_ratio_threshold}"
        }
        results.append(ret)
        print(f"  Found high ad ratio: {ad_ratio:.2f} ({ad_count}/{total_nodes})")

    # è§„åˆ™2: å¹¿å‘Šé¢‘ç¹å‡ºç° (å…è®¸ä¸€å®šçš„éå¹¿å‘Šé—´éš”)
    for nid, node in graph.state.items():
        if not node.get("is_ad_related", False):
            continue

        chain = [nid]  # å½“å‰é“¾
        current = nid
        visited = set()

        while True:
            visited.add(current)
            next_nodes = list(graph.state[current].get("dst", {}).keys())
            if not next_nodes:
                break

            found_next_ad = False
            for dst in next_nodes:
                if dst in visited:
                    continue

                # æ£€æŸ¥ dst åˆ°ä¸‹ä¸€ä¸ªå¹¿å‘Šçš„è·ç¦» (gap)
                path = [dst]
                gap = 0
                while gap <= gap_threshold:
                    if graph.state[path[-1]].get("is_ad_related", False):
                        # æ‰¾åˆ°ä¸‹ä¸€ä¸ªå¹¿å‘Š
                        chain.append(path[-1])
                        current = path[-1]
                        found_next_ad = True
                        break
                    # ç»§ç»­å¾€ä¸‹æ‰¾
                    next_dst = list(graph.state[path[-1]].get("dst", {}).keys())
                    if not next_dst:
                        break
                    path.append(next_dst[0])  # å–ç¬¬ä¸€ä¸ª
                    gap += 1

                if found_next_ad:
                    break

            if not found_next_ad:
                break

            if len(chain) >= consecutive_threshold:
                ret = {
                    "case": "frequent_ads",
                    "start_node": nid,
                    "chain_length": len(chain),
                    "gap_threshold": gap_threshold,
                    "pattern": " -> ".join(chain)
                }
                if ret not in results:
                    results.append(ret)
                print(f"  Found frequent ads chain: {' -> '.join(chain)} (len={len(chain)})")
                break

    return results

# def extract_paths_to_ads(graph, max_depth=20):
#     """
#     æå–ä»å…¥å£èŠ‚ç‚¹åˆ°å¹¿å‘Šé¡µé¢çš„æ‰€æœ‰åŠ¨ä½œåºåˆ—
#     ç»“æœå¯ä¾› DroidBot ç”Ÿæˆ input events ä½¿ç”¨

#     :param graph: UTG å›¾å¯¹è±¡ (åŒ…å« state, src, dst, edges)
#     :param max_depth: æœ€å¤§æœç´¢æ·±åº¦ï¼Œé¿å…æ­»å¾ªç¯
#     :return: List[List[Dict]]ï¼Œæ¯ä¸ªå­åˆ—è¡¨æ˜¯ä¸€æ¡äº‹ä»¶åºåˆ—
#     """
#     print("[+] Extracting paths to ads")

#     results = []

#     # å…¥å£èŠ‚ç‚¹ï¼ˆå‡è®¾æœ‰ root å­—æ®µï¼Œå¦åˆ™å– state ä¸­ç¬¬ä¸€ä¸ªï¼‰
#     if hasattr(graph, "root"):
#         start_node = graph.root
#     else:
#         start_node = list(graph.state.keys())[0]

#     # DFS æœç´¢è·¯å¾„
#     def dfs(current, path, events, depth):
#         if depth > max_depth:
#             return

#         node = graph.state[current]

#         # å¦‚æœåˆ°è¾¾å¹¿å‘Šé¡µé¢ï¼Œä¿å­˜è·¯å¾„
#         if node.get("is_ad_related", False):
#             results.append(events.copy())
#             print(f"  Found ad path: {[e['event_str'] for e in events]}")
#             return

#         # éå† outgoing edges
#         for dst, edge_info in node.get("dst", {}).items():
#             for e in edge_info.get("events", []):
#                 new_events = events + [e]
#                 dfs(dst, path + [dst], new_events, depth + 1)

#     dfs(start_node, [start_node], [], 0)
#     return results
        
# def check_type5(graph):
#     """
#     5. Outside-App Ads: - UI activities do not belong to the app
#     """
#     print("[+] checking type5 outside-app ads ...")
#     results = []
    
#     # é¦–å…ˆæ”¶é›†æ‰€æœ‰ä½œä¸ºå¹¿å‘ŠèŠ‚ç‚¹ç›®æ ‡çš„éå¹¿å‘ŠèŠ‚ç‚¹
#     ad_targets = set()
#     for edge in graph.edge:  # éå†è¾¹åˆ—è¡¨
#         # ç›´æ¥è®¿é—®è¾¹çš„å±æ€§ï¼Œè€Œä¸æ˜¯ä½¿ç”¨ get æ–¹æ³•
#         src_node_id = edge.src if hasattr(edge, 'src') else None
#         dst_node_id = edge.dst if hasattr(edge, 'dst') else None
        
#         if src_node_id and dst_node_id:
#             src_node = graph.state.get(src_node_id)
#             if src_node and src_node.get("is_ad_related", False):
#                 ad_targets.add(dst_node_id)
    
#     for node_id, node in graph.state.items():
#         # æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦æ¥è‡ªå¤–éƒ¨åº”ç”¨
#         package_name = node.get("package", "")
#         if ("launcher" in package_name or package_name.startswith("com.android")):
#             # æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦æ˜¯å¹¿å‘Šç›¸å…³ï¼Œæˆ–è€…æ˜¯å¹¿å‘ŠèŠ‚ç‚¹çš„ç›®æ ‡
#             if node.get("is_ad_related", False) or node_id in ad_targets:
#                 results.append({
#                     "node": node_id,
#                     "activity": node.get("activity"),
#                     "case": "outside-app ads",
#                     "pattern": f"{node_id}(ad) -- out of app"
#                 })
#                 print(f"[+] {node_id}(ad) -- out of app")

#     return results

def extract_paths_to_ads(graph, entry_state=None, max_depth=10, output_file=None):
    """
    ä»å…¥å£çŠ¶æ€å‡ºå‘ï¼Œæå–åˆ°å¹¿å‘Šé¡µé¢çš„æ‰€æœ‰è·¯å¾„
    :param graph: dynamic_graph å¯¹è±¡
    :param entry_state: æŒ‡å®šå…¥å£èŠ‚ç‚¹IDï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™å–ç¬¬ä¸€ä¸ªï¼‰
    :param max_depth: é™åˆ¶æœç´¢æ·±åº¦ï¼Œé¿å…ç¯è·¯
    :param output_file: å¦‚æœæŒ‡å®šï¼Œå°†ç»“æœä¿å­˜åˆ° JSON æ–‡ä»¶
    :return: æ‰€æœ‰è·¯å¾„å’Œäº‹ä»¶çš„åˆ—è¡¨
    """
    results = []
    visited = set()

    # å…¥å£èŠ‚ç‚¹
    if entry_state is None:
        if not graph.state:
            return []
        entry_state = list(graph.state.keys())[0]

    def dfs(current, path, events, depth):
        if depth > max_depth:
            return
        visited.add(current)
        path.append(current)

        node = graph.state[current]
        if node.get("is_ad_related") or node.get("is_external_site"):
            results.append({
                "path": path.copy(),
                "events": events.copy()
            })
            path.pop()
            visited.remove(current)
            return

        for neighbor, edge_info in node.get("dst", {}).items():
            if neighbor not in visited:
                new_events = events + edge_info.get("events", [])
                dfs(neighbor, path, new_events, depth + 1)

        path.pop()
        visited.remove(current)

    dfs(entry_state, [], [], 0)

    # ä¿å­˜åˆ°æ–‡ä»¶
    if output_file:
        import json
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"[+] Saved extracted paths to {output_file}")

    return results

def batch_analyze(output_dirs_with_csv, global_summary="global_summary.csv", sensor_input_csv="sensor_test_input_csv"):
    print("batch analysis")
    """
    æ‰¹é‡åˆ†æå¤šä¸ªè¾“å‡ºç›®å½•ã€‚

    å‚æ•°:
        output_dirs_with_csv: list[tuple[str, str]]
            ä¾‹å¦‚: [("/data/output_dir1", "log1.csv"), ("/data/output_dir2", "log2.csv")]
        global_summary: str
            æœ€ç»ˆæ±‡æ€»çš„CSVæ–‡ä»¶å
    """
    global_stats = []
    sensor_entries = []

    for (output_dir, csv_path) in output_dirs_with_csv:
        print(f"\n[+] å¼€å§‹åˆ†æ: {output_dir}")
        if not os.path.exists(output_dir):
            print(f"[!] è·³è¿‡ä¸å­˜åœ¨çš„ç›®å½•: {output_dir}")
            continue

        output_csv = os.path.join(output_dir, "apk_analysis_results.csv")
        result_stats = analyze(output_dir, output_csv)

        # ---- ç”Ÿæˆ JSON è¾“å‡º ----
        json_output = os.path.join(output_dir, "apk_analysis_results.json")
        convert_csv_to_json(output_csv, json_output)
        print(f"[âœ”] JSONç»“æœå†™å…¥: {json_output}")

        # ---- æ·»åŠ ä¸€æ¡å…¨å±€ç»Ÿè®¡ä¿¡æ¯ ----
        global_stats.append({
            "output_dir": output_dir,
            "csv_file": csv_path,
            "total_apks": result_stats.get("total_apks", 0),
            "apks_with_ads": result_stats.get("apks_with_ads", 0),
            "type2_count": result_stats.get("type2_count", 0),
            "type3_count": result_stats.get("type3_count", 0),
            "type4_count": result_stats.get("type4_count", 0),
            "type5_count": result_stats.get("type5_count", 0),
            "type6_count": result_stats.get("type6_count", 0),
            "failed_analysis": result_stats.get("failed_analysis", 0),
            "analyzed_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        })

        # ---- è§£æå½“å‰ç›®å½•çš„ has_ad åº”ç”¨ ----
        with open(output_csv, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                if row.get("has_ad", "").lower() in ("true", "1", "yes"):
                    sensor_entries.append({
                        "package_name": row.get("app_name", ""),
                        "apk_name": row.get("app_name", ""),
                        "output_dir": output_dir,
                        "has_ad": True,
                        "analyzed_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    })

    # ---- æ±‡æ€»æ‰€æœ‰åˆ†æç»“æœ ----
    if global_stats:
        write_global_summary(global_stats, global_summary)
        print(f"[âœ”] å…¨éƒ¨ä»»åŠ¡å®Œæˆï¼Œæ±‡æ€»ç»“æœå†™å…¥ï¼š{global_summary}")
    else:
        print("[!] æ²¡æœ‰æˆåŠŸåˆ†æä»»ä½•ç›®å½•ã€‚")
    
    # ---- æ‰“å°å…¨å±€ç»Ÿè®¡ ----
    print_global_stats()

    # ---- å†™å‡º sensor_test_input.csv ----
    if sensor_entries:
        write_sensor_input(sensor_entries, sensor_input_csv)
        print(f"[âœ”] å·²ç”Ÿæˆ sensor_test_input.csvï¼Œè®°å½• {len(sensor_entries)} ä¸ªå¹¿å‘Šç›¸å…³åº”ç”¨")
    else:
        print("[!] æ²¡æœ‰æ£€æµ‹åˆ°å¹¿å‘Šåº”ç”¨ï¼Œæœªç”Ÿæˆ sensor_test_input.csv")

def write_sensor_input(entries, output_csv):
    """å†™å‡º sensor æµ‹è¯•è¾“å…¥åˆ—è¡¨"""
    fieldnames = ["package_name", "apk_name", "output_dir", "has_ad", "analyzed_at"]
    with open(output_csv, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(entries)

def convert_csv_to_json(csv_file, json_file):
    """å°†åˆ†æç»“æœCSVè½¬æ¢ä¸ºJSONæ–‡ä»¶"""
    if not os.path.exists(csv_file):
        print(f"[!] CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file}")
        return
    data = []
    with open(csv_file, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            data.append(row)
    with open(json_file, "w", encoding="utf-8") as jf:
        json.dump(data, jf, ensure_ascii=False, indent=2)

def write_global_summary(records, output_csv):
    """å†™å‡ºæ±‡æ€»çš„å…¨å±€ç»Ÿè®¡ç»“æœ"""
    fieldnames = [
        "output_dir", "csv_file",
        "total_apks", "apks_with_ads",
        "type2_count", "type3_count",
        "type4_count", "type5_count",
        "type6_count", "failed_analysis",
        "analyzed_at"
    ]
    with open(output_csv, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(records)

# if __name__=='__main__':
    #path = os.path.join('examples/C07B41EB38A4AA087A9B2883AA8F3679C035441AD4470F2A23')
    # root_directory = "D:\\NKU\\Work\\Work2\\appchina_output"
    
    #path = os.path.join('examples/com.dz.law')
    #analyze(path)
    #analyze_test(path)
    
    # æŒ‡å®šåŒ…å«APKåˆ†æç»“æœçš„æ ¹ç›®å½•
    #root_directory = "D:\\NKU\\Work\\Work2\\datasets\\androzoo\\androzoo_output"
    
    # æŒ‡å®šè¾“å‡ºCSVæ–‡ä»¶è·¯å¾„
    # output_csv = "apk_analysis_results.csv"
    
    # è¿è¡Œåˆ†æ
    # analyze(root_directory, output_csv)

    # read a list of root

    '''
    dirs_to_analyze = [
        ("F:\\test\\output", "F:\\test\\merge_output.csv"),
        ("E:\\test\\output", "E:\\test\\untested_simulator1.csv"),
        ("D:\\NKU\\Work\\Work2\\appchina_output", "D:\\NKU\\Work\\Work2\\appchina_output\\log.csv"),
        ("D:\\NKU\\Work\\Work2\\datasets\\androzoo\\androzoo_output", "D:\\NKU\\Work\\Work2\\datasets\\aligned_log.csv")
    ]

    batch_analyze(dirs_to_analyze, global_summary="global_offline_summary.csv", sensor_input_csv="sensor_test_input.csv")
    '''

# å…¨å±€æ–‡ä»¶è·¯å¾„
GLOBAL_CSV = "all_apps_master.csv"
GLOBAL_TXT = "checked_apps.txt"

def ensure_global_files():
    """ç¡®ä¿å…¨å±€CSVå’ŒTXTæ–‡ä»¶å­˜åœ¨"""
    # CSVåˆ—å®šä¹‰ - ä¸åŸæœ‰åˆ†æç»“æœä¿æŒä¸€è‡´
    csv_columns = [
        'app_name', 'has_ad', 'type2_detected', 'type2_features',
        'type3_detected', 'type3_features', 'type4_detected', 'type4_features',
        'type5_detected', 'type5_features', 'type6_detected', 'type6_features',
        'analysis_date', 'app_path', 'is_tested', 'test_date', 'utg_exists'
    ]
    
    if not os.path.exists(GLOBAL_CSV):
        df = pd.DataFrame(columns=csv_columns)
        df.to_csv(GLOBAL_CSV, index=False)
        print(f"[+] åˆ›å»ºå…¨å±€CSV: {GLOBAL_CSV}")
    
    if not os.path.exists(GLOBAL_TXT):
        with open(GLOBAL_TXT, 'w', encoding='utf-8') as f:
            f.write("# å·²æ£€æµ‹åº”ç”¨åˆ—è¡¨\n")
        print(f"[+] åˆ›å»ºå…¨å±€TXT: {GLOBAL_TXT}")

def load_checked_apps():
    """åŠ è½½å·²æ£€æŸ¥çš„åº”ç”¨åˆ—è¡¨"""
    checked_apps = set()
    if os.path.exists(GLOBAL_TXT):
        with open(GLOBAL_TXT, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    checked_apps.add(line)
    return checked_apps

def incremental_analysis_with_full_detection(folders_to_scan):
    """
    å¢é‡åˆ†æä¸»å‡½æ•° - ä½¿ç”¨å®Œæ•´çš„å¹¿å‘Šæ£€æµ‹æµç¨‹
    
    Args:
        folders_to_scan: è¦æ‰«æçš„æ–‡ä»¶å¤¹è·¯å¾„åˆ—è¡¨
    """
    print("ğŸš€ å¼€å§‹å¢é‡åˆ†æï¼ˆå®Œæ•´æ£€æµ‹æµç¨‹ï¼‰...")
    
    # ç¡®ä¿å…¨å±€æ–‡ä»¶å­˜åœ¨
    ensure_global_files()
    
    # åŠ è½½å·²æ£€æŸ¥çš„åº”ç”¨
    checked_apps = load_checked_apps()
    print(f"[+] å·²åŠ è½½ {len(checked_apps)} ä¸ªå·²æ£€æŸ¥åº”ç”¨")
    
    # ç»Ÿè®¡
    stats = {
        'scanned_folders': 0,
        'new_apps': 0,
        'skipped_apps': 0,
        'failed_apps': 0
    }
    
    # éå†æ¯ä¸ªå¤§æ–‡ä»¶å¤¹
    for folder_path in folders_to_scan:
        if not os.path.exists(folder_path):
            print(f"[!] æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
            continue
            
        print(f"\n[+] æ‰«ææ–‡ä»¶å¤¹: {folder_path}")
        stats['scanned_folders'] += 1
        
        # è·å–æ‰€æœ‰å­æ–‡ä»¶å¤¹
        try:
            sub_folders = [f for f in os.listdir(folder_path) 
                          if os.path.isdir(os.path.join(folder_path, f))]
        except Exception as e:
            print(f"[!] è¯»å–æ–‡ä»¶å¤¹å¤±è´¥: {e}")
            continue
            
        print(f"    æ‰¾åˆ° {len(sub_folders)} ä¸ªå­æ–‡ä»¶å¤¹")
        
        # å¤„ç†æ¯ä¸ªå­æ–‡ä»¶å¤¹
        for app_name in sub_folders:
            app_folder = os.path.join(folder_path, app_name)
            
            # æ£€æŸ¥æ˜¯å¦å·²åˆ†æè¿‡
            if app_name in checked_apps:
                stats['skipped_apps'] += 1
                continue
                
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨utg.js
            utg_path = os.path.join(app_folder, "utg.js")
            if not os.path.exists(utg_path):
                stats['skipped_apps'] += 1
                continue
                
            print(f"\n[+] åˆ†ææ–°åº”ç”¨: {app_name}")
            
            # ä½¿ç”¨å®Œæ•´çš„åˆ†ææµç¨‹
            try:
                # è°ƒç”¨åŸæœ‰çš„å®Œæ•´åˆ†æå‡½æ•°
                result = analyze_single_apk(app_folder, app_name)
                
                if result:
                    # æ·»åŠ é¢å¤–ä¿¡æ¯
                    result['app_path'] = app_folder
                    result['is_tested'] = 'TRUE'
                    result['test_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    result['utg_exists'] = 'TRUE'
                    
                    # æ›´æ–°å…¨å±€æ–‡ä»¶
                    if update_global_files_with_full_result(result):
                        stats['new_apps'] += 1
                        print(f"[+] æˆåŠŸåˆ†æå¹¶è®°å½•: {app_name}")
                    else:
                        stats['failed_apps'] += 1
                        print(f"[!] è®°å½•å¤±è´¥: {app_name}")
                else:
                    stats['failed_apps'] += 1
                    print(f"[!] åˆ†æå¤±è´¥: {app_name}")
                    
            except Exception as e:
                stats['failed_apps'] += 1
                print(f"[!] åˆ†æå¼‚å¸¸ {app_name}: {e}")
    
    # è¾“å‡ºç»Ÿè®¡
    print(f"\nğŸ“Š åˆ†æå®Œæˆç»Ÿè®¡:")
    print(f"   æ‰«ææ–‡ä»¶å¤¹: {stats['scanned_folders']}")
    print(f"   æ–°å¢åº”ç”¨: {stats['new_apps']}")
    print(f"   è·³è¿‡åº”ç”¨: {stats['skipped_apps']}")
    print(f"   å¤±è´¥åº”ç”¨: {stats['failed_apps']}")
    print(f"   å…¨å±€CSV: {GLOBAL_CSV}")
    print(f"   å…¨å±€TXT: {GLOBAL_TXT}")
    
    return stats

def update_global_files_with_full_result(result):
    """ä½¿ç”¨å®Œæ•´åˆ†æç»“æœæ›´æ–°å…¨å±€æ–‡ä»¶"""
    try:
        # è¯»å–ç°æœ‰CSV
        if os.path.exists(GLOBAL_CSV) and os.path.getsize(GLOBAL_CSV) > 0:
            df = pd.read_csv(GLOBAL_CSV, dtype=str).fillna("")
        else:
            df = pd.DataFrame(columns=[
                'app_name', 'has_ad', 'type2_detected', 'type2_features',
                'type3_detected', 'type3_features', 'type4_detected', 'type4_features',
                'type5_detected', 'type5_features', 'type6_detected', 'type6_features',
                'analysis_date', 'app_path', 'is_tested', 'test_date', 'utg_exists'
            ])
        
        app_name = result['app_name']
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        existing_mask = df['app_name'] == app_name
        
        if existing_mask.any():
            # æ›´æ–°ç°æœ‰è®°å½•
            for idx in df[existing_mask].index:
                for col, value in result.items():
                    if col in df.columns and value:
                        df.loc[idx, col] = value
            print(f"   æ›´æ–°è®°å½•: {app_name}")
        else:
            # æ·»åŠ æ–°è®°å½•
            new_row = {col: '' for col in df.columns}
            new_row.update(result)
            df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)
            print(f"   æ–°å¢è®°å½•: {app_name}")
        
        # ä¿å­˜CSV
        df.to_csv(GLOBAL_CSV, index=False)
        
        # æ›´æ–°TXTæ–‡ä»¶
        checked_apps = load_checked_apps()
        checked_apps.add(app_name)
        with open(GLOBAL_TXT, 'w', encoding='utf-8') as f:
            f.write("# å·²æ£€æµ‹åº”ç”¨åˆ—è¡¨\n")
            for app in sorted(checked_apps):
                f.write(f"{app}\n")
        
        return True
        
    except Exception as e:
        print(f"[!] æ›´æ–°å…¨å±€æ–‡ä»¶å¤±è´¥: {e}")
        return False

def get_analysis_stats():
    """è·å–åˆ†æç»Ÿè®¡ä¿¡æ¯"""
    try:
        if not os.path.exists(GLOBAL_CSV):
            return {"total_apps": 0, "tested_apps": 0}
        
        df = pd.read_csv(GLOBAL_CSV, dtype=str).fillna("")
        
        total_apps = len(df)
        tested_apps = len(df[df['is_tested'] == 'TRUE'])
        has_ad = len(df[df['has_ad'] == 'TRUE'])
        
        # ç»Ÿè®¡å„ç§å¹¿å‘Šç±»å‹
        type2_count = len(df[df['type2_detected'] == 'TRUE'])
        type3_count = len(df[df['type3_detected'] == 'TRUE'])
        type4_count = len(df[df['type4_detected'] == 'TRUE'])
        type5_count = len(df[df['type5_detected'] == 'TRUE'])
        type6_count = len(df[df['type6_detected'] == 'TRUE'])
        
        return {
            "total_apps": total_apps,
            "tested_apps": tested_apps,
            "has_ad": has_ad,
            "type2_count": type2_count,
            "type3_count": type3_count,
            "type4_count": type4_count,
            "type5_count": type5_count,
            "type6_count": type6_count,
            "ad_percentage": (has_ad / total_apps * 100) if total_apps > 0 else 0
        }
    except Exception as e:
        print(f"[!] è·å–ç»Ÿè®¡å¤±è´¥: {e}")
        return {"total_apps": 0, "tested_apps": 0}

def print_detailed_stats():
    """æ‰“å°è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
    stats = get_analysis_stats()
    
    print(f"\nğŸ“ˆ è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   æ€»åº”ç”¨æ•°: {stats['total_apps']}")
    print(f"   å·²æµ‹è¯•åº”ç”¨: {stats['tested_apps']}")
    print(f"   åŒ…å«å¹¿å‘Š: {stats['has_ad']} ({stats['ad_percentage']:.1f}%)")
    print(f"   å¹¿å‘Šç±»å‹åˆ†å¸ƒ:")
    print(f"     - Type2: {stats['type2_count']}")
    print(f"     - Type3: {stats['type3_count']}")
    print(f"     - Type4: {stats['type4_count']}")
    print(f"     - Type5: {stats['type5_count']}")
    print(f"     - Type6: {stats['type6_count']}")
    print(f"   å…¨å±€CSV: {GLOBAL_CSV}")
    print(f"   å…¨å±€TXT: {GLOBAL_TXT}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    folders_to_scan = [
        "D:\\NKU\\Work\\Work2\\fraudulent_output",
        "D:\\NKU\\Work\\Work2\\datasets\\manual_analysis\\output", 
        "D:\\NKU\\Work\\Work2\\datasets\\chin\\output",
        "D:\\NKU\\Work\\Work2\\datasets\\manual_analysis\\test_adgpe_test"
    ]
    
    # æ‰§è¡Œå¢é‡åˆ†æï¼ˆä½¿ç”¨å®Œæ•´æ£€æµ‹æµç¨‹ï¼‰
    stats = incremental_analysis_with_full_detection(folders_to_scan)
    
    # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    print_detailed_stats()
