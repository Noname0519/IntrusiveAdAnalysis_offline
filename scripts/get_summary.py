import os
import pandas as pd
import json
from datetime import datetime

# å…¨å±€æ–‡ä»¶è·¯å¾„
GLOBAL_CSV = "all_apps_master.csv"
GLOBAL_TXT = "checked_apps.txt"

def deduplicate_and_analyze_master_csv():
    """
    å¯¹å…¨å±€CSVæ–‡ä»¶è¿›è¡Œå»é‡å’Œç»Ÿè®¡åˆ†æ
    
    Returns:
        dict: åŒ…å«è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    try:
        if not os.path.exists(GLOBAL_CSV):
            print(f"[!] å…¨å±€CSVæ–‡ä»¶ä¸å­˜åœ¨: {GLOBAL_CSV}")
            return None
        
        # è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv(GLOBAL_CSV, dtype=str).fillna("")
        original_count = len(df)
        print(f"[+] è¯»å–å…¨å±€CSV: å…± {original_count} æ¡è®°å½•")
        
        if original_count == 0:
            print("[!] CSVæ–‡ä»¶ä¸ºç©º")
            return None
        
        # å»é‡å¤„ç† - åŸºäºapp_nameï¼Œä¿ç•™æœ€æ–°çš„è®°å½•
        df['test_date'] = pd.to_datetime(df['test_date'], errors='coerce')
        df_sorted = df.sort_values('test_date', ascending=False)
        df_dedup = df_sorted.drop_duplicates('app_name', keep='first')
        dedup_count = len(df_dedup)
        
        print(f"[+] å»é‡å®Œæˆ: ä» {original_count} æ¡è®°å½•å»é‡åˆ° {dedup_count} æ¡")
        
        # å¦‚æœæœ‰é‡å¤ï¼Œä¿å­˜å»é‡åçš„æ–‡ä»¶
        if dedup_count < original_count:
            backup_file = GLOBAL_CSV.replace(".csv", f"_backup_{int(datetime.now().timestamp())}.csv")
            df.to_csv(backup_file, index=False)
            print(f"[+] å·²å¤‡ä»½åŸæ–‡ä»¶: {backup_file}")
            
            df_dedup.to_csv(GLOBAL_CSV, index=False)
            print(f"[+] å·²ä¿å­˜å»é‡åçš„æ–‡ä»¶: {GLOBAL_CSV}")
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats = calculate_detailed_stats(df_dedup)
        
        # ä¿å­˜ç»Ÿè®¡æŠ¥å‘Š
        save_statistics_report(stats, df_dedup)
        
        return stats
        
    except Exception as e:
        print(f"[!] å¤„ç†å…¨å±€CSVå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def calculate_detailed_stats(df):
    """
    è®¡ç®—è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        df: å»é‡åçš„DataFrame
        
    Returns:
        dict: è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    """
    # åŸºæœ¬ç»Ÿè®¡
    total_apps = len(df)
    tested_apps = len(df[df['is_tested'].str.upper().isin(['TRUE', 'T', '1', 'YES', 'Y'])])
    utg_exists = len(df[df['utg_exists'].str.upper().isin(['TRUE', 'T', '1', 'YES', 'Y'])])
    has_ad = len(df[df['has_ad'].str.upper().isin(['TRUE', 'T', '1', 'YES', 'Y'])])
    
    # å¹¿å‘Šç±»å‹ç»Ÿè®¡
    type2_count = len(df[df['type2_detected'].str.upper().isin(['TRUE', 'T', '1', 'YES', 'Y'])])
    type3_count = len(df[df['type3_detected'].str.upper().isin(['TRUE', 'T', '1', 'YES', 'Y'])])
    type4_count = len(df[df['type4_detected'].str.upper().isin(['TRUE', 'T', '1', 'YES', 'Y'])])
    type5_count = len(df[df['type5_detected'].str.upper().isin(['TRUE', 'T', '1', 'YES', 'Y'])])
    type6_count = len(df[df['type6_detected'].str.upper().isin(['TRUE', 'T', '1', 'YES', 'Y'])])
    
    # è®¡ç®—ç™¾åˆ†æ¯”
    ad_percentage = (has_ad / total_apps * 100) if total_apps > 0 else 0
    tested_percentage = (tested_apps / total_apps * 100) if total_apps > 0 else 0
    utg_percentage = (utg_exists / total_apps * 100) if total_apps > 0 else 0
    
    # åœ¨å¹¿å‘Šåº”ç”¨ä¸­çš„ç±»å‹åˆ†å¸ƒ
    type2_in_ad = (type2_count / has_ad * 100) if has_ad > 0 else 0
    type3_in_ad = (type3_count / has_ad * 100) if has_ad > 0 else 0
    type4_in_ad = (type4_count / has_ad * 100) if has_ad > 0 else 0
    type5_in_ad = (type5_count / has_ad * 100) if has_ad > 0 else 0
    type6_in_ad = (type6_count / has_ad * 100) if has_ad > 0 else 0
    
    # æ—¶é—´èŒƒå›´ç»Ÿè®¡
    if 'test_date' in df.columns and not df['test_date'].isnull().all():
        df['test_date'] = pd.to_datetime(df['test_date'], errors='coerce')
        date_range = df['test_date'].dropna()
        if len(date_range) > 0:
            earliest_date = date_range.min().strftime('%Y-%m-%d')
            latest_date = date_range.max().strftime('%Y-%m-%d')
        else:
            earliest_date = latest_date = "N/A"
    else:
        earliest_date = latest_date = "N/A"
    
    stats = {
        "total_apps": total_apps,
        "tested_apps": tested_apps,
        "utg_exists": utg_exists,
        "has_ad": has_ad,
        "type2_count": type2_count,
        "type3_count": type3_count,
        "type4_count": type4_count,
        "type5_count": type5_count,
        "type6_count": type6_count,
        "ad_percentage": ad_percentage,
        "tested_percentage": tested_percentage,
        "utg_percentage": utg_percentage,
        "type2_in_ad": type2_in_ad,
        "type3_in_ad": type3_in_ad,
        "type4_in_ad": type4_in_ad,
        "type5_in_ad": type5_in_ad,
        "type6_in_ad": type6_in_ad,
        "earliest_test_date": earliest_date,
        "latest_test_date": latest_date,
        "analysis_timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }
    
    return stats

def save_statistics_report(stats, df):
    """
    ä¿å­˜ç»Ÿè®¡æŠ¥å‘Šåˆ°æ–‡ä»¶
    
    Args:
        stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        df: æ•°æ®DataFrame
    """
    try:
        report_file = GLOBAL_CSV.replace(".csv", "_statistics_report.txt")
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write("           APKåˆ†æç»Ÿè®¡æŠ¥å‘Š\n")
            f.write("=" * 60 + "\n\n")
            
            f.write(f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {stats['analysis_timestamp']}\n")
            f.write(f"æ•°æ®æ–‡ä»¶: {GLOBAL_CSV}\n")
            f.write(f"æµ‹è¯•æ—¶é—´èŒƒå›´: {stats['earliest_test_date']} è‡³ {stats['latest_test_date']}\n\n")
            
            f.write("ğŸ“Š æ€»ä½“ç»Ÿè®¡:\n")
            f.write("-" * 40 + "\n")
            f.write(f"æ€»åº”ç”¨æ•°: {stats['total_apps']}\n")
            f.write(f"å·²æµ‹è¯•åº”ç”¨: {stats['tested_apps']} ({stats['tested_percentage']:.1f}%)\n")
            f.write(f"æœ‰UTGæ–‡ä»¶: {stats['utg_exists']} ({stats['utg_percentage']:.1f}%)\n")
            f.write(f"åŒ…å«å¹¿å‘Š: {stats['has_ad']} ({stats['ad_percentage']:.1f}%)\n\n")
            
            f.write("ğŸ¯ å¹¿å‘Šç±»å‹åˆ†å¸ƒ:\n")
            f.write("-" * 40 + "\n")
            f.write(f"Type2 (åŠŸèƒ½æ€§ä¸­æ–­): {stats['type2_count']} ({stats['type2_in_ad']:.1f}% of ad apps)\n")
            f.write(f"Type3 (è¿”å›é”®é—®é¢˜): {stats['type3_count']} ({stats['type3_in_ad']:.1f}% of ad apps)\n")
            f.write(f"Type4 (é‡å®šå‘): {stats['type4_count']} ({stats['type4_in_ad']:.1f}% of ad apps)\n")
            f.write(f"Type5 (å¤–éƒ¨åº”ç”¨å¹¿å‘Š): {stats['type5_count']} ({stats['type5_in_ad']:.1f}% of ad apps)\n")
            f.write(f"Type6 (å¹¿å‘Šé¢‘ç‡): {stats['type6_count']} ({stats['type6_in_ad']:.1f}% of ad apps)\n\n")
            
            f.write("ğŸ“ˆ å¹¿å‘Šåº”ç”¨è¯¦æƒ…:\n")
            f.write("-" * 40 + "\n")
            
            # åˆ—å‡ºåŒ…å«å¹¿å‘Šçš„åº”ç”¨
            ad_apps = df[df['has_ad'].str.upper().isin(['TRUE', 'T', '1', 'YES', 'Y'])]
            if len(ad_apps) > 0:
                f.write("åŒ…å«å¹¿å‘Šçš„åº”ç”¨åˆ—è¡¨:\n")
                for _, app in ad_apps.iterrows():
                    app_name = app.get('app_name', 'Unknown')
                    types = []
                    if app.get('type2_detected', '').upper() in ['TRUE', 'T', '1', 'YES', 'Y']:
                        types.append("Type2")
                    if app.get('type3_detected', '').upper() in ['TRUE', 'T', '1', 'YES', 'Y']:
                        types.append("Type3")
                    if app.get('type4_detected', '').upper() in ['TRUE', 'T', '1', 'YES', 'Y']:
                        types.append("Type4")
                    if app.get('type5_detected', '').upper() in ['TRUE', 'T', '1', 'YES', 'Y']:
                        types.append("Type5")
                    if app.get('type6_detected', '').upper() in ['TRUE', 'T', '1', 'YES', 'Y']:
                        types.append("Type6")
                    
                    type_str = ", ".join(types) if types else "æ— å…·ä½“ç±»å‹"
                    f.write(f"  - {app_name}: {type_str}\n")
            else:
                f.write("æœªå‘ç°åŒ…å«å¹¿å‘Šçš„åº”ç”¨\n")
        
        print(f"[+] ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        return report_file
        
    except Exception as e:
        print(f"[!] ä¿å­˜ç»Ÿè®¡æŠ¥å‘Šå¤±è´¥: {e}")
        return None

def print_detailed_stats():
    """æ‰“å°è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯"""
    try:
        if not os.path.exists(GLOBAL_CSV):
            print(f"[!] å…¨å±€CSVæ–‡ä»¶ä¸å­˜åœ¨: {GLOBAL_CSV}")
            return
        
        # è¯»å–æ•°æ®
        df = pd.read_csv(GLOBAL_CSV, dtype=str).fillna("")
        
        if len(df) == 0:
            print("[!] CSVæ–‡ä»¶ä¸ºç©º")
            return
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats = calculate_detailed_stats(df)
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print("\n" + "=" * 60)
        print("           APKåˆ†æè¯¦ç»†ç»Ÿè®¡")
        print("=" * 60)
        
        print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        print(f"   æ€»åº”ç”¨æ•°: {stats['total_apps']}")
        print(f"   å·²æµ‹è¯•åº”ç”¨: {stats['tested_apps']} ({stats['tested_percentage']:.1f}%)")
        print(f"   æœ‰UTGæ–‡ä»¶: {stats['utg_exists']} ({stats['utg_percentage']:.1f}%)")
        print(f"   åŒ…å«å¹¿å‘Š: {stats['has_ad']} ({stats['ad_percentage']:.1f}%)")
        
        print(f"\nğŸ¯ å¹¿å‘Šç±»å‹åˆ†å¸ƒ (åœ¨{stats['has_ad']}ä¸ªå¹¿å‘Šåº”ç”¨ä¸­):")
        print(f"   Type2 (åŠŸèƒ½æ€§ä¸­æ–­): {stats['type2_count']} ({stats['type2_in_ad']:.1f}%)")
        print(f"   Type3 (è¿”å›é”®é—®é¢˜): {stats['type3_count']} ({stats['type3_in_ad']:.1f}%)")
        print(f"   Type4 (é‡å®šå‘): {stats['type4_count']} ({stats['type4_in_ad']:.1f}%)")
        print(f"   Type5 (å¤–éƒ¨åº”ç”¨å¹¿å‘Š): {stats['type5_count']} ({stats['type5_in_ad']:.1f}%)")
        print(f"   Type6 (å¹¿å‘Šé¢‘ç‡): {stats['type6_count']} ({stats['type6_in_ad']:.1f}%)")
        
        print(f"\nğŸ“… æµ‹è¯•æ—¶é—´èŒƒå›´:")
        print(f"   æœ€æ—©æµ‹è¯•: {stats['earliest_test_date']}")
        print(f"   æœ€æ–°æµ‹è¯•: {stats['latest_test_date']}")
        
        print(f"\nğŸ’¾ æ•°æ®æ–‡ä»¶:")
        print(f"   å…¨å±€CSV: {GLOBAL_CSV}")
        print(f"   å…¨å±€TXT: {GLOBAL_TXT}")
        
    except Exception as e:
        print(f"[!] æ‰“å°ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")

def get_master_csv_info():
    """
    è·å–ä¸»CSVæ–‡ä»¶çš„åŸºæœ¬ä¿¡æ¯
    
    Returns:
        dict: åŒ…å«æ–‡ä»¶ä¿¡æ¯çš„å­—å…¸
    """
    try:
        if not os.path.exists(GLOBAL_CSV):
            return {"exists": False, "size": 0, "record_count": 0}
        
        file_size = os.path.getsize(GLOBAL_CSV)
        
        df = pd.read_csv(GLOBAL_CSV, dtype=str).fillna("")
        record_count = len(df)
        
        return {
            "exists": True,
            "size": file_size,
            "record_count": record_count,
            "file_path": GLOBAL_CSV
        }
    except Exception as e:
        print(f"[!] è·å–CSVä¿¡æ¯å¤±è´¥: {e}")
        return {"exists": False, "size": 0, "record_count": 0}

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æ£€æŸ¥CSVæ–‡ä»¶ä¿¡æ¯
    csv_info = get_master_csv_info()
    if csv_info["exists"]:
        print(f"[+] ä¸»CSVæ–‡ä»¶: {csv_info['file_path']}")
        print(f"[+] æ–‡ä»¶å¤§å°: {csv_info['size']} å­—èŠ‚")
        print(f"[+] è®°å½•æ•°é‡: {csv_info['record_count']}")
        
        # å»é‡å’Œç»Ÿè®¡åˆ†æ
        stats = deduplicate_and_analyze_master_csv()
        
        if stats:
            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            print_detailed_stats()
    else:
        print(f"[!] ä¸»CSVæ–‡ä»¶ä¸å­˜åœ¨: {GLOBAL_CSV}")